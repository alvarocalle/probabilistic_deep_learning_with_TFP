{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c03479ce-3468-475c-8db2-664c172ac8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, MeanSquaredError\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232441d6-be03-4b75-877e-ce5f526f1dc0",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adea4ee-3542-44b8-833c-794a3eede8ef",
   "metadata": {},
   "source": [
    "This article belongs to the series \"Probabilistic Deep Learning\". This weekly series covers probabilistic approaches to deep learning. The main goal is to extend deep learning models to quantify uncertainty, i.e. know what they do not know. \n",
    "\n",
    "We develop our models using TensorFlow and TensorFlow Probability (TFP). TFP is a Python library built on top of TensorFlow. We are going to start with the basic objects that we can find in TensorFlow Probability (TFP) and understand how can we manipulate them. We will increase complexity incrementally over the following weeks and combine our probabilistic models with deep learning on modern hardware (e.g. GPU).\n",
    "\n",
    "\n",
    "\n",
    "As usual, the code is available on my GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca224abd-b32d-4942-8de6-dcf8ad742239",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3b12a3-f9a7-4334-95d4-f73947774b3c",
   "metadata": {},
   "source": [
    "Let's recall what we shared at the end of the last article regarding Maximum Likelihood Estimation (MLE).\n",
    "\n",
    "MLE is the usual training procedure used in deep learning models. The goal is to estimate the parameters of a probability distribution, given some data. In very simple terms, we want to maximize the probability of the data that we observed under some assumed statistical model, i.e. probability distribution.\n",
    "\n",
    "We also introduced some notation. The probability density function of a continuous random variable roughly indicates the probability of a sample taking a particular value. We will denote this function $P(x | \\theta)$ where $x$ is the value of the sample and $\\theta$ is the parameter describing the probability distribution:\n",
    "\n",
    "$$\n",
    "P(x | \\theta) = \\text{Prob} (\\text{sampling value $x$ from a distribution with parameter $\\theta$}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41da673b-77ed-42de-87f1-9c2c278b7ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.3989423>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfd.Normal(0, 1).prob(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d3d3d-838b-4b00-af6e-1f0afd744676",
   "metadata": {},
   "source": [
    "It may seem fancy, but in fact, we have been computing the PDF of Gaussian distributions for a while now, so nothing particularly new here.\n",
    "\n",
    "When more than one sample is drawn *independently* from the same distribution (which we usually assume), the probability density function of the sample values $x_1, \\ldots, x_n$ is the product of the probability density functions for each individual $x_i$. Written formally:\n",
    "\n",
    "$$\n",
    "P(x_1, \\ldots, x_n | \\theta) = \\prod_{i=1}^n P(x_i | \\theta).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946fef79-9ebc-4c8a-a756-01a4a3ae4665",
   "metadata": {},
   "source": [
    "We can easily compute the above with an example. Imagine that we have a standard Gaussian distribution and some samples: $x_1=-0.5$, $x_2=0$ and $x_3=1.5$. As we defined above, we just need to compute the PDF of each sample and multiply the output together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21fef5c7-9474-47c4-81ed-5272728cef94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01819123"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [-0.5, 0, 1.5]\n",
    "\n",
    "np.prod(tfd.Normal(0, 1).prob(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b40cd-7cb6-47d3-86a9-f60a69b37085",
   "metadata": {},
   "source": [
    "Now, I want to somehow give some intuition about the differences between a probability density function a likelihood function. They are essentially computing similar things, but with opposite perspectives.\n",
    "\n",
    "Starting with the probability density function, we know that they are functions of our samples $x_1, \\ldots, x_n$. Notice that the parameter $\\theta$ is considered fixed. So, the probability density function is used when the parameter $\\theta$ is known and our interest is to find out the probabilities of same samples $x_1, \\ldots, x_n$. In simple terms, we use this function when we know the distribution that generated some process and we want to make deductions about possible values sampled from it.\n",
    "\n",
    "Conversely, in the case of the *likelihood* function, what is known to us are the samples, i.e. the observed data $x_1, \\ldots, x_n$. This means that our independent variable is now $\\theta$, since we do not know which distribution generated this process that we observed. Hence, we use this function when the samples of some process are known to us, i.e. we collected data, but we do not really what distribution generated the process in the first place. Since we know the data, we are interested to make *inferences* about the distribution that they came from.\n",
    "\n",
    "Let's introduce some more notation to help connect the dots. For the *likelihood* function, the convention is to use the letter $L$, while for the probability density function we introduced the notation above. We can then write the following:\n",
    "\n",
    "$$\n",
    "\\underbrace{L(x_1, \\ldots, x_n | \\theta)}_{\\text{ likelihood,} \\\\ \\text{function of $\\theta$}} = \\underbrace{P(x_1, \\ldots, x_n | \\theta)}_{\\text{probabiliy density,} \\\\ \\text{ function of $x_1, \\ldots, x_n$}}\n",
    "$$\n",
    "\n",
    "We are ready to define our likelihood function for the Gaussian distribution with parameters $\\mu$ and $\\sigma$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "L = f(X|\\theta) &= f(x_1|\\theta) f(x_2|\\theta),..., f(x_n|\\theta) \\\\\n",
    "&= \\prod^n_{j=1}f(X| \\mu,\\sigma^2) \\\\\n",
    "&= (2\\pi\\sigma^2)^{-n/2} \\exp{\\big(-\\frac{1}{2\\sigma^2} \\sum^n_{j=1}(x_i-\\mu)^2\\big)}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d57b50b-a954-4f3d-b510-60e45b2c38b5",
   "metadata": {},
   "source": [
    "As an exercise to get some more intuition about the *likelihood* function, we can generate enough samples to get a visual glimpse of its shape. Notice the difference of computing the *likelihood* function when compared to the computation of the probability density function that we did in the last article. We are not interested in generating samples from a probability distribution, we are interested in generating parameters $\\theta$ that maximize the probability of the observed data, i.e. $P(x_1, \\ldots, x_n | \\theta) = \\prod_{i=1}^n P(x_i | \\theta)$.\n",
    "\n",
    "We are using the same samples as above $x_1=-0.5$, $x_2=0$ and $x_3=1.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d54ae23-e755-4641-9345-3660dea96053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.5, 0, 1.5]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27426f91-3d2b-46b9-b931-6c73d633201b",
   "metadata": {},
   "source": [
    "To be able to build a 2D visualization, we can create a grid of potential parameters sampled evenly spaced over an interval, $\\mu$ is sampled from [-2,2] and $\\sigma$ between [0,3]. Since we sampled 100 values for each parameter we got $n^2$ possible combinations. For each combination of parameters, we need to compute the probability of each sample and multiply them together (following the procedure that we shared above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25dbef58-a17f-4081-8690-f4e8bdd983c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "μ = np.linspace(-2, 2, 100)\n",
    "σ = np.linspace(0, 3, 100)\n",
    "\n",
    "l_x = []\n",
    "for mu in μ:\n",
    "    for sigma in σ:\n",
    "        l_x.append(np.prod(tfd.Normal(mu, sigma).prob(X)))\n",
    "        \n",
    "l_x = np.asarray(l_x).reshape((100, 100)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a7974-7989-4a07-bad4-1d0a9142e8e9",
   "metadata": {},
   "source": [
    "We are now ready to plot the *likelihood* function. Note that it is function of the observed samples and Recall that these are fixed, the parameters are our independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53752704-dd79-47d0-afbc-19beb763297e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.contourf(μ, σ, l_x)\n",
    "plt.xlabel('μ')\n",
    "plt.ylabel('σ')\n",
    "plt.colorbar()\n",
    "plt.title('Likelihood');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215e70d-2131-4416-8e93-218233aad861",
   "metadata": {},
   "source": [
    "As we already shared, we are interested in maximizing the probability of our data. This means that we want to find the maximum value of the *likelihood* function, which can be achieved with a bit of calculus. In fact, the zeros of the first derivative of our function with respect to our parameters should be sufficient to help us find the maximum of the original function. We now get to a new problem that we already introduced in a previous article - multiplying many small probabilities together can be numerically unstable.  To overcome this problem, we can use the log transformation of the same function. The natural logarithm is a monotonically increasing function, which means that if the value on the x-axis increases, the value on the y-axis also increases. This is important because it ensures that the maximum value of the log of the probability occurs at the same point as the original probability function. It does another very conveniently thing for us, it transforms our products into sums.\n",
    "\n",
    "Let's perform the transformation:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\log(L(X|\\theta)) &= \\log\\big((2\\pi\\sigma^2)^{-n/2} \\exp{\\big(-\\frac{1}{2\\sigma^2} \\sum^n_{j=1}(y_i-\\mu)^2\\big)\\big)} \\\\\n",
    "&= -\\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}\\log(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{j=1}^{n}(y_i-\\mu)^2\n",
    "\\end{aligned}$$\n",
    "\n",
    "We are almost there, we can now work our optimization problem at hand. Maximizing the probability of our data can be written as:\n",
    "\n",
    "$$\\max_{\\mu,\\sigma^2}\\log(L(X|\\theta))$$\n",
    "\n",
    "As we stated above, the expression derived above can be differentiated to find the maximum. Expanding our parameters we have $\\log(L(X|\\mu, \\sigma))$. As it is a function of the two variables $\\mu$ and $\\sigma$ we use partial derivatives to find the *Maximum Likelihood Estimation*. \n",
    "\n",
    "Let's focus on $\\hat \\mu$ (the hat indicates that it is an estimator, i.e. our output), we compute it from\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& \\quad \\frac{\\partial}{\\partial \\mu} \\log(L(Y|\\mu, \\sigma)) \\\\\n",
    "&= \\frac{\\partial}{\\partial \\mu} \\big(-\\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}\\log(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{j=1}^{n}(x_i-\\mu)^2\\big)\n",
    "\\\\\n",
    "&= \\sum^n_{j=1} \\frac{(x_i - \\mu)}{\\sigma^2}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Setting the expression above equal to zero we get\n",
    "\n",
    "\n",
    "$$\\sum^n_{j=1} \\frac{(x_i - \\mu)}{\\sigma^2} = 0 $$\n",
    "\n",
    "Then\n",
    "$$\\begin{aligned}\n",
    "\\hat\\mu &= \\frac{\\sum^n_{j=1}x_i}{n} \\\\\n",
    "\\hat\\mu &= \\bar x\n",
    "\\end{aligned}$$\n",
    "\n",
    "It should not be a surprise to you that this is the mean of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133e21eb-3b1f-471b-a451-a1073748d508",
   "metadata": {},
   "source": [
    "We can compute the maximum values for the μ and σ for our samples $x_1=-0.5$, $x_2=0$ and $x_3=1.5$ and compare with the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b01f0b3-4892-4e14-92bd-6a4098f5c6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "μ True Value: 0.3333333333333333\n",
      "μ Calculated Value: 0.3434343434343434\n",
      "σ True Value: 0.8498365855987975\n",
      "σ Calculated Value: 0.8484848484848485\n"
     ]
    }
   ],
   "source": [
    "idx_μ_max = np.argmax(l_x, axis=1)[-1]\n",
    "print(f'μ True Value: {np.array(X).mean()}')\n",
    "print(f'μ Calculated Value: {μ[idx_μ_max]}')\n",
    "print(f'σ True Value: {np.array(X).std()}')\n",
    "print(f'σ Calculated Value: {σ[np.nanargmax(l_x[:,idx_μ_max], axis=0)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ae59b-26a2-4754-a2b6-42ba4771957f",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation implementation in TensorFlow Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32cd582-234c-433c-b9af-dcef9065f342",
   "metadata": {},
   "source": [
    "Let's start by creating a random variable normally distributed and sample from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53def4ad-a45a-48e3-b09b-316d3acbfed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.normal(loc=1, scale=5, size=1000).astype('float32')[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcaf8578-2c86-4108-a518-c36f679e994a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a histogram of the random variable\n",
    "\n",
    "plt.hist(x_train, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7b65574-99fb-4376-a1c0-3f5c80019dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85486585"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean of the samples of the random variable\n",
    "\n",
    "x_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71396aaa-5c9c-4a25-ba03-3a877c73cc5d",
   "metadata": {},
   "source": [
    "As we saw in the last article, we can define TensorFlow `Variable` objects as the parameters of our distribution. This signals to TensorFlow that we want to learn these parameters during the learning procedure, whichever we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baee9b72-3e65-412f-89a1-e3508235d09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'loc:0' shape=() dtype=float32, numpy=0.0>,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal = tfd.Normal(loc=tf.Variable(0., name='loc'), scale=5)\n",
    "normal.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7346d8bd-0af3-4494-90d2-c8458d972ae2",
   "metadata": {},
   "source": [
    "The next step if to define our loss function. In this case, we already saw what we want to achieve - maximize our log transformation of the *likelihood* function. However, in Deep Learning we usually minimize our loss functions, which can be easily achieved if we change the sign of our *likelihood* function to be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "570494c3-b5ba-4193-92b4-8525dc16bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the negative log likelihood function (loss function)\n",
    "\n",
    "def nll(x_train):\n",
    "    return -tf.reduce_sum(normal.log_prob(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c95e7b-81e8-469c-ac2b-675060d29638",
   "metadata": {},
   "source": [
    "Finally, we can build our training procedure. We will use a custom training loop, so that we can define the procedure details ourselves (namely use our custom loss function).\n",
    "\n",
    "As we also shared in a previous article, we use `tf.GradientTape()` which is the API to access the automatic differentiation features of TensorFlow. Next, we simply specify what are the variables to train, the loss function to minimize and apply the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ccd11988-fe92-4e55-bf92-e347114222ff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom training loop\n",
    "\n",
    "@tf.function\n",
    "def get_loss_and_grads(x_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(normal.trainable_variables)\n",
    "        loss = nll(x_train)\n",
    "        grads = tape.gradient(loss, normal.trainable_variables)\n",
    "    return loss, grads\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897220f-b920-441a-aa91-c7d4a8256827",
   "metadata": {},
   "source": [
    "We are ready to run our training procedure for some epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30991a90-92fe-41fc-a7f6-f8853de15627",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 000: Loss: 13768.004 Loc: 0.855\n",
      "Step 001: Loss: 13768.004 Loc: 0.855\n",
      "Step 002: Loss: 13768.004 Loc: 0.855\n",
      "Step 003: Loss: 13768.004 Loc: 0.855\n",
      "Step 004: Loss: 13768.004 Loc: 0.855\n",
      "Step 005: Loss: 13768.004 Loc: 0.855\n",
      "Step 006: Loss: 13768.004 Loc: 0.855\n",
      "Step 007: Loss: 13768.004 Loc: 0.855\n",
      "Step 008: Loss: 13768.004 Loc: 0.855\n",
      "Step 009: Loss: 13768.004 Loc: 0.855\n",
      "Step 010: Loss: 13768.004 Loc: 0.855\n",
      "Step 011: Loss: 13768.004 Loc: 0.855\n",
      "Step 012: Loss: 13768.004 Loc: 0.855\n",
      "Step 013: Loss: 13768.004 Loc: 0.855\n",
      "Step 014: Loss: 13768.004 Loc: 0.855\n",
      "Step 015: Loss: 13768.004 Loc: 0.855\n",
      "Step 016: Loss: 13768.004 Loc: 0.855\n",
      "Step 017: Loss: 13768.004 Loc: 0.855\n",
      "Step 018: Loss: 13768.004 Loc: 0.855\n",
      "Step 019: Loss: 13768.004 Loc: 0.855\n",
      "Step 020: Loss: 13768.004 Loc: 0.855\n",
      "Step 021: Loss: 13768.004 Loc: 0.855\n",
      "Step 022: Loss: 13768.004 Loc: 0.855\n",
      "Step 023: Loss: 13768.004 Loc: 0.855\n",
      "Step 024: Loss: 13768.004 Loc: 0.855\n",
      "Step 025: Loss: 13768.004 Loc: 0.855\n",
      "Step 026: Loss: 13768.004 Loc: 0.855\n",
      "Step 027: Loss: 13768.004 Loc: 0.855\n",
      "Step 028: Loss: 13768.004 Loc: 0.855\n",
      "Step 029: Loss: 13768.004 Loc: 0.855\n",
      "Step 030: Loss: 13768.004 Loc: 0.855\n",
      "Step 031: Loss: 13768.004 Loc: 0.855\n",
      "Step 032: Loss: 13768.004 Loc: 0.855\n",
      "Step 033: Loss: 13768.004 Loc: 0.855\n",
      "Step 034: Loss: 13768.004 Loc: 0.855\n",
      "Step 035: Loss: 13768.004 Loc: 0.855\n",
      "Step 036: Loss: 13768.004 Loc: 0.855\n",
      "Step 037: Loss: 13768.004 Loc: 0.855\n",
      "Step 038: Loss: 13768.004 Loc: 0.855\n",
      "Step 039: Loss: 13768.004 Loc: 0.855\n",
      "Step 040: Loss: 13768.004 Loc: 0.855\n",
      "Step 041: Loss: 13768.004 Loc: 0.855\n",
      "Step 042: Loss: 13768.004 Loc: 0.855\n",
      "Step 043: Loss: 13768.004 Loc: 0.855\n",
      "Step 044: Loss: 13768.004 Loc: 0.855\n",
      "Step 045: Loss: 13768.004 Loc: 0.855\n",
      "Step 046: Loss: 13768.004 Loc: 0.855\n",
      "Step 047: Loss: 13768.004 Loc: 0.855\n",
      "Step 048: Loss: 13768.004 Loc: 0.855\n",
      "Step 049: Loss: 13768.004 Loc: 0.855\n",
      "Step 050: Loss: 13768.004 Loc: 0.855\n",
      "Step 051: Loss: 13768.004 Loc: 0.855\n",
      "Step 052: Loss: 13768.004 Loc: 0.855\n",
      "Step 053: Loss: 13768.004 Loc: 0.855\n",
      "Step 054: Loss: 13768.004 Loc: 0.855\n",
      "Step 055: Loss: 13768.004 Loc: 0.855\n",
      "Step 056: Loss: 13768.004 Loc: 0.855\n",
      "Step 057: Loss: 13768.004 Loc: 0.855\n",
      "Step 058: Loss: 13768.004 Loc: 0.855\n",
      "Step 059: Loss: 13768.004 Loc: 0.855\n",
      "Step 060: Loss: 13768.004 Loc: 0.855\n",
      "Step 061: Loss: 13768.004 Loc: 0.855\n",
      "Step 062: Loss: 13768.004 Loc: 0.855\n",
      "Step 063: Loss: 13768.004 Loc: 0.855\n",
      "Step 064: Loss: 13768.004 Loc: 0.855\n",
      "Step 065: Loss: 13768.004 Loc: 0.855\n",
      "Step 066: Loss: 13768.004 Loc: 0.855\n",
      "Step 067: Loss: 13768.004 Loc: 0.855\n",
      "Step 068: Loss: 13768.004 Loc: 0.855\n",
      "Step 069: Loss: 13768.004 Loc: 0.855\n",
      "Step 070: Loss: 13768.004 Loc: 0.855\n",
      "Step 071: Loss: 13768.004 Loc: 0.855\n",
      "Step 072: Loss: 13768.004 Loc: 0.855\n",
      "Step 073: Loss: 13768.004 Loc: 0.855\n",
      "Step 074: Loss: 13768.004 Loc: 0.855\n",
      "Step 075: Loss: 13768.004 Loc: 0.855\n",
      "Step 076: Loss: 13768.004 Loc: 0.855\n",
      "Step 077: Loss: 13768.004 Loc: 0.855\n",
      "Step 078: Loss: 13768.004 Loc: 0.855\n",
      "Step 079: Loss: 13768.004 Loc: 0.855\n",
      "Step 080: Loss: 13768.004 Loc: 0.855\n",
      "Step 081: Loss: 13768.004 Loc: 0.855\n",
      "Step 082: Loss: 13768.004 Loc: 0.855\n",
      "Step 083: Loss: 13768.004 Loc: 0.855\n",
      "Step 084: Loss: 13768.004 Loc: 0.855\n",
      "Step 085: Loss: 13768.004 Loc: 0.855\n",
      "Step 086: Loss: 13768.004 Loc: 0.855\n",
      "Step 087: Loss: 13768.004 Loc: 0.855\n",
      "Step 088: Loss: 13768.004 Loc: 0.855\n",
      "Step 089: Loss: 13768.004 Loc: 0.855\n",
      "Step 090: Loss: 13768.004 Loc: 0.855\n",
      "Step 091: Loss: 13768.004 Loc: 0.855\n",
      "Step 092: Loss: 13768.004 Loc: 0.855\n",
      "Step 093: Loss: 13768.004 Loc: 0.855\n",
      "Step 094: Loss: 13768.004 Loc: 0.855\n",
      "Step 095: Loss: 13768.004 Loc: 0.855\n",
      "Step 096: Loss: 13768.004 Loc: 0.855\n",
      "Step 097: Loss: 13768.004 Loc: 0.855\n",
      "Step 098: Loss: 13768.004 Loc: 0.855\n",
      "Step 099: Loss: 13768.004 Loc: 0.855\n",
      "Step 100: Loss: 13768.004 Loc: 0.855\n",
      "Step 101: Loss: 13768.004 Loc: 0.855\n",
      "Step 102: Loss: 13768.004 Loc: 0.855\n",
      "Step 103: Loss: 13768.004 Loc: 0.855\n",
      "Step 104: Loss: 13768.004 Loc: 0.855\n",
      "Step 105: Loss: 13768.004 Loc: 0.855\n",
      "Step 106: Loss: 13768.004 Loc: 0.855\n",
      "Step 107: Loss: 13768.004 Loc: 0.855\n",
      "Step 108: Loss: 13768.004 Loc: 0.855\n",
      "Step 109: Loss: 13768.004 Loc: 0.855\n",
      "Step 110: Loss: 13768.004 Loc: 0.855\n",
      "Step 111: Loss: 13768.004 Loc: 0.855\n",
      "Step 112: Loss: 13768.004 Loc: 0.855\n",
      "Step 113: Loss: 13768.004 Loc: 0.855\n",
      "Step 114: Loss: 13768.004 Loc: 0.855\n",
      "Step 115: Loss: 13768.004 Loc: 0.855\n",
      "Step 116: Loss: 13768.004 Loc: 0.855\n",
      "Step 117: Loss: 13768.004 Loc: 0.855\n",
      "Step 118: Loss: 13768.004 Loc: 0.855\n",
      "Step 119: Loss: 13768.004 Loc: 0.855\n",
      "Step 120: Loss: 13768.004 Loc: 0.855\n",
      "Step 121: Loss: 13768.004 Loc: 0.855\n",
      "Step 122: Loss: 13768.004 Loc: 0.855\n",
      "Step 123: Loss: 13768.004 Loc: 0.855\n",
      "Step 124: Loss: 13768.004 Loc: 0.855\n",
      "Step 125: Loss: 13768.004 Loc: 0.855\n",
      "Step 126: Loss: 13768.004 Loc: 0.855\n",
      "Step 127: Loss: 13768.004 Loc: 0.855\n",
      "Step 128: Loss: 13768.004 Loc: 0.855\n",
      "Step 129: Loss: 13768.004 Loc: 0.855\n",
      "Step 130: Loss: 13768.004 Loc: 0.855\n",
      "Step 131: Loss: 13768.004 Loc: 0.855\n",
      "Step 132: Loss: 13768.004 Loc: 0.855\n",
      "Step 133: Loss: 13768.004 Loc: 0.855\n",
      "Step 134: Loss: 13768.004 Loc: 0.855\n",
      "Step 135: Loss: 13768.004 Loc: 0.855\n",
      "Step 136: Loss: 13768.004 Loc: 0.855\n",
      "Step 137: Loss: 13768.004 Loc: 0.855\n",
      "Step 138: Loss: 13768.004 Loc: 0.855\n",
      "Step 139: Loss: 13768.004 Loc: 0.855\n",
      "Step 140: Loss: 13768.004 Loc: 0.855\n",
      "Step 141: Loss: 13768.004 Loc: 0.855\n",
      "Step 142: Loss: 13768.004 Loc: 0.855\n",
      "Step 143: Loss: 13768.004 Loc: 0.855\n",
      "Step 144: Loss: 13768.004 Loc: 0.855\n",
      "Step 145: Loss: 13768.004 Loc: 0.855\n",
      "Step 146: Loss: 13768.004 Loc: 0.855\n",
      "Step 147: Loss: 13768.004 Loc: 0.855\n",
      "Step 148: Loss: 13768.004 Loc: 0.855\n",
      "Step 149: Loss: 13768.004 Loc: 0.855\n",
      "Step 150: Loss: 13768.004 Loc: 0.855\n",
      "Step 151: Loss: 13768.004 Loc: 0.855\n",
      "Step 152: Loss: 13768.004 Loc: 0.855\n",
      "Step 153: Loss: 13768.004 Loc: 0.855\n",
      "Step 154: Loss: 13768.004 Loc: 0.855\n",
      "Step 155: Loss: 13768.004 Loc: 0.855\n",
      "Step 156: Loss: 13768.004 Loc: 0.855\n",
      "Step 157: Loss: 13768.004 Loc: 0.855\n",
      "Step 158: Loss: 13768.004 Loc: 0.855\n",
      "Step 159: Loss: 13768.004 Loc: 0.855\n",
      "Step 160: Loss: 13768.004 Loc: 0.855\n",
      "Step 161: Loss: 13768.004 Loc: 0.855\n",
      "Step 162: Loss: 13768.004 Loc: 0.855\n",
      "Step 163: Loss: 13768.004 Loc: 0.855\n",
      "Step 164: Loss: 13768.004 Loc: 0.855\n",
      "Step 165: Loss: 13768.004 Loc: 0.855\n",
      "Step 166: Loss: 13768.004 Loc: 0.855\n",
      "Step 167: Loss: 13768.004 Loc: 0.855\n",
      "Step 168: Loss: 13768.004 Loc: 0.855\n",
      "Step 169: Loss: 13768.004 Loc: 0.855\n",
      "Step 170: Loss: 13768.004 Loc: 0.855\n",
      "Step 171: Loss: 13768.004 Loc: 0.855\n",
      "Step 172: Loss: 13768.004 Loc: 0.855\n",
      "Step 173: Loss: 13768.004 Loc: 0.855\n",
      "Step 174: Loss: 13768.004 Loc: 0.855\n",
      "Step 175: Loss: 13768.004 Loc: 0.855\n",
      "Step 176: Loss: 13768.004 Loc: 0.855\n",
      "Step 177: Loss: 13768.004 Loc: 0.855\n",
      "Step 178: Loss: 13768.004 Loc: 0.855\n",
      "Step 179: Loss: 13768.004 Loc: 0.855\n",
      "Step 180: Loss: 13768.004 Loc: 0.855\n",
      "Step 181: Loss: 13768.004 Loc: 0.855\n",
      "Step 182: Loss: 13768.004 Loc: 0.855\n",
      "Step 183: Loss: 13768.004 Loc: 0.855\n",
      "Step 184: Loss: 13768.004 Loc: 0.855\n",
      "Step 185: Loss: 13768.004 Loc: 0.855\n",
      "Step 186: Loss: 13768.004 Loc: 0.855\n",
      "Step 187: Loss: 13768.004 Loc: 0.855\n",
      "Step 188: Loss: 13768.004 Loc: 0.855\n",
      "Step 189: Loss: 13768.004 Loc: 0.855\n",
      "Step 190: Loss: 13768.004 Loc: 0.855\n",
      "Step 191: Loss: 13768.004 Loc: 0.855\n",
      "Step 192: Loss: 13768.004 Loc: 0.855\n",
      "Step 193: Loss: 13768.004 Loc: 0.855\n",
      "Step 194: Loss: 13768.004 Loc: 0.855\n",
      "Step 195: Loss: 13768.004 Loc: 0.855\n",
      "Step 196: Loss: 13768.004 Loc: 0.855\n",
      "Step 197: Loss: 13768.004 Loc: 0.855\n",
      "Step 198: Loss: 13768.004 Loc: 0.855\n",
      "Step 199: Loss: 13768.004 Loc: 0.855\n",
      "Step 200: Loss: 13768.004 Loc: 0.855\n",
      "Step 201: Loss: 13768.004 Loc: 0.855\n",
      "Step 202: Loss: 13768.004 Loc: 0.855\n",
      "Step 203: Loss: 13768.004 Loc: 0.855\n",
      "Step 204: Loss: 13768.004 Loc: 0.855\n",
      "Step 205: Loss: 13768.004 Loc: 0.855\n",
      "Step 206: Loss: 13768.004 Loc: 0.855\n",
      "Step 207: Loss: 13768.004 Loc: 0.855\n",
      "Step 208: Loss: 13768.004 Loc: 0.855\n",
      "Step 209: Loss: 13768.004 Loc: 0.855\n",
      "Step 210: Loss: 13768.004 Loc: 0.855\n",
      "Step 211: Loss: 13768.004 Loc: 0.855\n",
      "Step 212: Loss: 13768.004 Loc: 0.855\n",
      "Step 213: Loss: 13768.004 Loc: 0.855\n",
      "Step 214: Loss: 13768.004 Loc: 0.855\n",
      "Step 215: Loss: 13768.004 Loc: 0.855\n",
      "Step 216: Loss: 13768.004 Loc: 0.855\n",
      "Step 217: Loss: 13768.004 Loc: 0.855\n",
      "Step 218: Loss: 13768.004 Loc: 0.855\n",
      "Step 219: Loss: 13768.004 Loc: 0.855\n",
      "Step 220: Loss: 13768.004 Loc: 0.855\n",
      "Step 221: Loss: 13768.004 Loc: 0.855\n",
      "Step 222: Loss: 13768.004 Loc: 0.855\n",
      "Step 223: Loss: 13768.004 Loc: 0.855\n",
      "Step 224: Loss: 13768.004 Loc: 0.855\n",
      "Step 225: Loss: 13768.004 Loc: 0.855\n",
      "Step 226: Loss: 13768.004 Loc: 0.855\n",
      "Step 227: Loss: 13768.004 Loc: 0.855\n",
      "Step 228: Loss: 13768.004 Loc: 0.855\n",
      "Step 229: Loss: 13768.004 Loc: 0.855\n",
      "Step 230: Loss: 13768.004 Loc: 0.855\n",
      "Step 231: Loss: 13768.004 Loc: 0.855\n",
      "Step 232: Loss: 13768.004 Loc: 0.855\n",
      "Step 233: Loss: 13768.004 Loc: 0.855\n",
      "Step 234: Loss: 13768.004 Loc: 0.855\n",
      "Step 235: Loss: 13768.004 Loc: 0.855\n",
      "Step 236: Loss: 13768.004 Loc: 0.855\n",
      "Step 237: Loss: 13768.004 Loc: 0.855\n",
      "Step 238: Loss: 13768.004 Loc: 0.855\n",
      "Step 239: Loss: 13768.004 Loc: 0.855\n",
      "Step 240: Loss: 13768.004 Loc: 0.855\n",
      "Step 241: Loss: 13768.004 Loc: 0.855\n",
      "Step 242: Loss: 13768.004 Loc: 0.855\n",
      "Step 243: Loss: 13768.004 Loc: 0.855\n",
      "Step 244: Loss: 13768.004 Loc: 0.855\n",
      "Step 245: Loss: 13768.004 Loc: 0.855\n",
      "Step 246: Loss: 13768.004 Loc: 0.855\n",
      "Step 247: Loss: 13768.004 Loc: 0.855\n",
      "Step 248: Loss: 13768.004 Loc: 0.855\n",
      "Step 249: Loss: 13768.004 Loc: 0.855\n",
      "Step 250: Loss: 13768.004 Loc: 0.855\n",
      "Step 251: Loss: 13768.004 Loc: 0.855\n",
      "Step 252: Loss: 13768.004 Loc: 0.855\n",
      "Step 253: Loss: 13768.004 Loc: 0.855\n",
      "Step 254: Loss: 13768.004 Loc: 0.855\n",
      "Step 255: Loss: 13768.004 Loc: 0.855\n",
      "Step 256: Loss: 13768.004 Loc: 0.855\n",
      "Step 257: Loss: 13768.004 Loc: 0.855\n",
      "Step 258: Loss: 13768.004 Loc: 0.855\n",
      "Step 259: Loss: 13768.004 Loc: 0.855\n",
      "Step 260: Loss: 13768.004 Loc: 0.855\n",
      "Step 261: Loss: 13768.004 Loc: 0.855\n",
      "Step 262: Loss: 13768.004 Loc: 0.855\n",
      "Step 263: Loss: 13768.004 Loc: 0.855\n",
      "Step 264: Loss: 13768.004 Loc: 0.855\n",
      "Step 265: Loss: 13768.004 Loc: 0.855\n",
      "Step 266: Loss: 13768.004 Loc: 0.855\n",
      "Step 267: Loss: 13768.004 Loc: 0.855\n",
      "Step 268: Loss: 13768.004 Loc: 0.855\n",
      "Step 269: Loss: 13768.004 Loc: 0.855\n",
      "Step 270: Loss: 13768.004 Loc: 0.855\n",
      "Step 271: Loss: 13768.004 Loc: 0.855\n",
      "Step 272: Loss: 13768.004 Loc: 0.855\n",
      "Step 273: Loss: 13768.004 Loc: 0.855\n",
      "Step 274: Loss: 13768.004 Loc: 0.855\n",
      "Step 275: Loss: 13768.004 Loc: 0.855\n",
      "Step 276: Loss: 13768.004 Loc: 0.855\n",
      "Step 277: Loss: 13768.004 Loc: 0.855\n",
      "Step 278: Loss: 13768.004 Loc: 0.855\n",
      "Step 279: Loss: 13768.004 Loc: 0.855\n",
      "Step 280: Loss: 13768.004 Loc: 0.855\n",
      "Step 281: Loss: 13768.004 Loc: 0.855\n",
      "Step 282: Loss: 13768.004 Loc: 0.855\n",
      "Step 283: Loss: 13768.004 Loc: 0.855\n",
      "Step 284: Loss: 13768.004 Loc: 0.855\n",
      "Step 285: Loss: 13768.004 Loc: 0.855\n",
      "Step 286: Loss: 13768.004 Loc: 0.855\n",
      "Step 287: Loss: 13768.004 Loc: 0.855\n",
      "Step 288: Loss: 13768.004 Loc: 0.855\n",
      "Step 289: Loss: 13768.004 Loc: 0.855\n",
      "Step 290: Loss: 13768.004 Loc: 0.855\n",
      "Step 291: Loss: 13768.004 Loc: 0.855\n",
      "Step 292: Loss: 13768.004 Loc: 0.855\n",
      "Step 293: Loss: 13768.004 Loc: 0.855\n",
      "Step 294: Loss: 13768.004 Loc: 0.855\n",
      "Step 295: Loss: 13768.004 Loc: 0.855\n",
      "Step 296: Loss: 13768.004 Loc: 0.855\n",
      "Step 297: Loss: 13768.004 Loc: 0.855\n",
      "Step 298: Loss: 13768.004 Loc: 0.855\n",
      "Step 299: Loss: 13768.004 Loc: 0.855\n",
      "Step 300: Loss: 13768.004 Loc: 0.855\n",
      "Step 301: Loss: 13768.004 Loc: 0.855\n",
      "Step 302: Loss: 13768.004 Loc: 0.855\n",
      "Step 303: Loss: 13768.004 Loc: 0.855\n",
      "Step 304: Loss: 13768.004 Loc: 0.855\n",
      "Step 305: Loss: 13768.004 Loc: 0.855\n",
      "Step 306: Loss: 13768.004 Loc: 0.855\n",
      "Step 307: Loss: 13768.004 Loc: 0.855\n",
      "Step 308: Loss: 13768.004 Loc: 0.855\n",
      "Step 309: Loss: 13768.004 Loc: 0.855\n",
      "Step 310: Loss: 13768.004 Loc: 0.855\n",
      "Step 311: Loss: 13768.004 Loc: 0.855\n",
      "Step 312: Loss: 13768.004 Loc: 0.855\n",
      "Step 313: Loss: 13768.004 Loc: 0.855\n",
      "Step 314: Loss: 13768.004 Loc: 0.855\n",
      "Step 315: Loss: 13768.004 Loc: 0.855\n",
      "Step 316: Loss: 13768.004 Loc: 0.855\n",
      "Step 317: Loss: 13768.004 Loc: 0.855\n",
      "Step 318: Loss: 13768.004 Loc: 0.855\n",
      "Step 319: Loss: 13768.004 Loc: 0.855\n",
      "Step 320: Loss: 13768.004 Loc: 0.855\n",
      "Step 321: Loss: 13768.004 Loc: 0.855\n",
      "Step 322: Loss: 13768.004 Loc: 0.855\n",
      "Step 323: Loss: 13768.004 Loc: 0.855\n",
      "Step 324: Loss: 13768.004 Loc: 0.855\n",
      "Step 325: Loss: 13768.004 Loc: 0.855\n",
      "Step 326: Loss: 13768.004 Loc: 0.855\n",
      "Step 327: Loss: 13768.004 Loc: 0.855\n",
      "Step 328: Loss: 13768.004 Loc: 0.855\n",
      "Step 329: Loss: 13768.004 Loc: 0.855\n",
      "Step 330: Loss: 13768.004 Loc: 0.855\n",
      "Step 331: Loss: 13768.004 Loc: 0.855\n",
      "Step 332: Loss: 13768.004 Loc: 0.855\n",
      "Step 333: Loss: 13768.004 Loc: 0.855\n",
      "Step 334: Loss: 13768.004 Loc: 0.855\n",
      "Step 335: Loss: 13768.004 Loc: 0.855\n",
      "Step 336: Loss: 13768.004 Loc: 0.855\n",
      "Step 337: Loss: 13768.004 Loc: 0.855\n",
      "Step 338: Loss: 13768.004 Loc: 0.855\n",
      "Step 339: Loss: 13768.004 Loc: 0.855\n",
      "Step 340: Loss: 13768.004 Loc: 0.855\n",
      "Step 341: Loss: 13768.004 Loc: 0.855\n",
      "Step 342: Loss: 13768.004 Loc: 0.855\n",
      "Step 343: Loss: 13768.004 Loc: 0.855\n",
      "Step 344: Loss: 13768.004 Loc: 0.855\n",
      "Step 345: Loss: 13768.004 Loc: 0.855\n",
      "Step 346: Loss: 13768.004 Loc: 0.855\n",
      "Step 347: Loss: 13768.004 Loc: 0.855\n",
      "Step 348: Loss: 13768.004 Loc: 0.855\n",
      "Step 349: Loss: 13768.004 Loc: 0.855\n",
      "Step 350: Loss: 13768.004 Loc: 0.855\n",
      "Step 351: Loss: 13768.004 Loc: 0.855\n",
      "Step 352: Loss: 13768.004 Loc: 0.855\n",
      "Step 353: Loss: 13768.004 Loc: 0.855\n",
      "Step 354: Loss: 13768.004 Loc: 0.855\n",
      "Step 355: Loss: 13768.004 Loc: 0.855\n",
      "Step 356: Loss: 13768.004 Loc: 0.855\n",
      "Step 357: Loss: 13768.004 Loc: 0.855\n",
      "Step 358: Loss: 13768.004 Loc: 0.855\n",
      "Step 359: Loss: 13768.004 Loc: 0.855\n",
      "Step 360: Loss: 13768.004 Loc: 0.855\n",
      "Step 361: Loss: 13768.004 Loc: 0.855\n",
      "Step 362: Loss: 13768.004 Loc: 0.855\n",
      "Step 363: Loss: 13768.004 Loc: 0.855\n",
      "Step 364: Loss: 13768.004 Loc: 0.855\n",
      "Step 365: Loss: 13768.004 Loc: 0.855\n",
      "Step 366: Loss: 13768.004 Loc: 0.855\n",
      "Step 367: Loss: 13768.004 Loc: 0.855\n",
      "Step 368: Loss: 13768.004 Loc: 0.855\n",
      "Step 369: Loss: 13768.004 Loc: 0.855\n",
      "Step 370: Loss: 13768.004 Loc: 0.855\n",
      "Step 371: Loss: 13768.004 Loc: 0.855\n",
      "Step 372: Loss: 13768.004 Loc: 0.855\n",
      "Step 373: Loss: 13768.004 Loc: 0.855\n",
      "Step 374: Loss: 13768.004 Loc: 0.855\n",
      "Step 375: Loss: 13768.004 Loc: 0.855\n",
      "Step 376: Loss: 13768.004 Loc: 0.855\n",
      "Step 377: Loss: 13768.004 Loc: 0.855\n",
      "Step 378: Loss: 13768.004 Loc: 0.855\n",
      "Step 379: Loss: 13768.004 Loc: 0.855\n",
      "Step 380: Loss: 13768.004 Loc: 0.855\n",
      "Step 381: Loss: 13768.004 Loc: 0.855\n",
      "Step 382: Loss: 13768.004 Loc: 0.855\n",
      "Step 383: Loss: 13768.004 Loc: 0.855\n",
      "Step 384: Loss: 13768.004 Loc: 0.855\n",
      "Step 385: Loss: 13768.004 Loc: 0.855\n",
      "Step 386: Loss: 13768.004 Loc: 0.855\n",
      "Step 387: Loss: 13768.004 Loc: 0.855\n",
      "Step 388: Loss: 13768.004 Loc: 0.855\n",
      "Step 389: Loss: 13768.004 Loc: 0.855\n",
      "Step 390: Loss: 13768.004 Loc: 0.855\n",
      "Step 391: Loss: 13768.004 Loc: 0.855\n",
      "Step 392: Loss: 13768.004 Loc: 0.855\n",
      "Step 393: Loss: 13768.004 Loc: 0.855\n",
      "Step 394: Loss: 13768.004 Loc: 0.855\n",
      "Step 395: Loss: 13768.004 Loc: 0.855\n",
      "Step 396: Loss: 13768.004 Loc: 0.855\n",
      "Step 397: Loss: 13768.004 Loc: 0.855\n",
      "Step 398: Loss: 13768.004 Loc: 0.855\n",
      "Step 399: Loss: 13768.004 Loc: 0.855\n",
      "Step 400: Loss: 13768.004 Loc: 0.855\n",
      "Step 401: Loss: 13768.004 Loc: 0.855\n",
      "Step 402: Loss: 13768.004 Loc: 0.855\n",
      "Step 403: Loss: 13768.004 Loc: 0.855\n",
      "Step 404: Loss: 13768.004 Loc: 0.855\n",
      "Step 405: Loss: 13768.004 Loc: 0.855\n",
      "Step 406: Loss: 13768.004 Loc: 0.855\n",
      "Step 407: Loss: 13768.004 Loc: 0.855\n",
      "Step 408: Loss: 13768.004 Loc: 0.855\n",
      "Step 409: Loss: 13768.004 Loc: 0.855\n",
      "Step 410: Loss: 13768.004 Loc: 0.855\n",
      "Step 411: Loss: 13768.004 Loc: 0.855\n",
      "Step 412: Loss: 13768.004 Loc: 0.855\n",
      "Step 413: Loss: 13768.004 Loc: 0.855\n",
      "Step 414: Loss: 13768.004 Loc: 0.855\n",
      "Step 415: Loss: 13768.004 Loc: 0.855\n",
      "Step 416: Loss: 13768.004 Loc: 0.855\n",
      "Step 417: Loss: 13768.004 Loc: 0.855\n",
      "Step 418: Loss: 13768.004 Loc: 0.855\n",
      "Step 419: Loss: 13768.004 Loc: 0.855\n",
      "Step 420: Loss: 13768.004 Loc: 0.855\n",
      "Step 421: Loss: 13768.004 Loc: 0.855\n",
      "Step 422: Loss: 13768.004 Loc: 0.855\n",
      "Step 423: Loss: 13768.004 Loc: 0.855\n",
      "Step 424: Loss: 13768.004 Loc: 0.855\n",
      "Step 425: Loss: 13768.004 Loc: 0.855\n",
      "Step 426: Loss: 13768.004 Loc: 0.855\n",
      "Step 427: Loss: 13768.004 Loc: 0.855\n",
      "Step 428: Loss: 13768.004 Loc: 0.855\n",
      "Step 429: Loss: 13768.004 Loc: 0.855\n",
      "Step 430: Loss: 13768.004 Loc: 0.855\n",
      "Step 431: Loss: 13768.004 Loc: 0.855\n",
      "Step 432: Loss: 13768.004 Loc: 0.855\n",
      "Step 433: Loss: 13768.004 Loc: 0.855\n",
      "Step 434: Loss: 13768.004 Loc: 0.855\n",
      "Step 435: Loss: 13768.004 Loc: 0.855\n",
      "Step 436: Loss: 13768.004 Loc: 0.855\n",
      "Step 437: Loss: 13768.004 Loc: 0.855\n",
      "Step 438: Loss: 13768.004 Loc: 0.855\n",
      "Step 439: Loss: 13768.004 Loc: 0.855\n",
      "Step 440: Loss: 13768.004 Loc: 0.855\n",
      "Step 441: Loss: 13768.004 Loc: 0.855\n",
      "Step 442: Loss: 13768.004 Loc: 0.855\n",
      "Step 443: Loss: 13768.004 Loc: 0.855\n",
      "Step 444: Loss: 13768.004 Loc: 0.855\n",
      "Step 445: Loss: 13768.004 Loc: 0.855\n",
      "Step 446: Loss: 13768.004 Loc: 0.855\n",
      "Step 447: Loss: 13768.004 Loc: 0.855\n",
      "Step 448: Loss: 13768.004 Loc: 0.855\n",
      "Step 449: Loss: 13768.004 Loc: 0.855\n",
      "Step 450: Loss: 13768.004 Loc: 0.855\n",
      "Step 451: Loss: 13768.004 Loc: 0.855\n",
      "Step 452: Loss: 13768.004 Loc: 0.855\n",
      "Step 453: Loss: 13768.004 Loc: 0.855\n",
      "Step 454: Loss: 13768.004 Loc: 0.855\n",
      "Step 455: Loss: 13768.004 Loc: 0.855\n",
      "Step 456: Loss: 13768.004 Loc: 0.855\n",
      "Step 457: Loss: 13768.004 Loc: 0.855\n",
      "Step 458: Loss: 13768.004 Loc: 0.855\n",
      "Step 459: Loss: 13768.004 Loc: 0.855\n",
      "Step 460: Loss: 13768.004 Loc: 0.855\n",
      "Step 461: Loss: 13768.004 Loc: 0.855\n",
      "Step 462: Loss: 13768.004 Loc: 0.855\n",
      "Step 463: Loss: 13768.004 Loc: 0.855\n",
      "Step 464: Loss: 13768.004 Loc: 0.855\n",
      "Step 465: Loss: 13768.004 Loc: 0.855\n",
      "Step 466: Loss: 13768.004 Loc: 0.855\n",
      "Step 467: Loss: 13768.004 Loc: 0.855\n",
      "Step 468: Loss: 13768.004 Loc: 0.855\n",
      "Step 469: Loss: 13768.004 Loc: 0.855\n",
      "Step 470: Loss: 13768.004 Loc: 0.855\n",
      "Step 471: Loss: 13768.004 Loc: 0.855\n",
      "Step 472: Loss: 13768.004 Loc: 0.855\n",
      "Step 473: Loss: 13768.004 Loc: 0.855\n",
      "Step 474: Loss: 13768.004 Loc: 0.855\n",
      "Step 475: Loss: 13768.004 Loc: 0.855\n",
      "Step 476: Loss: 13768.004 Loc: 0.855\n",
      "Step 477: Loss: 13768.004 Loc: 0.855\n",
      "Step 478: Loss: 13768.004 Loc: 0.855\n",
      "Step 479: Loss: 13768.004 Loc: 0.855\n",
      "Step 480: Loss: 13768.004 Loc: 0.855\n",
      "Step 481: Loss: 13768.004 Loc: 0.855\n",
      "Step 482: Loss: 13768.004 Loc: 0.855\n",
      "Step 483: Loss: 13768.004 Loc: 0.855\n",
      "Step 484: Loss: 13768.004 Loc: 0.855\n",
      "Step 485: Loss: 13768.004 Loc: 0.855\n",
      "Step 486: Loss: 13768.004 Loc: 0.855\n",
      "Step 487: Loss: 13768.004 Loc: 0.855\n",
      "Step 488: Loss: 13768.004 Loc: 0.855\n",
      "Step 489: Loss: 13768.004 Loc: 0.855\n",
      "Step 490: Loss: 13768.004 Loc: 0.855\n",
      "Step 491: Loss: 13768.004 Loc: 0.855\n",
      "Step 492: Loss: 13768.004 Loc: 0.855\n",
      "Step 493: Loss: 13768.004 Loc: 0.855\n",
      "Step 494: Loss: 13768.004 Loc: 0.855\n",
      "Step 495: Loss: 13768.004 Loc: 0.855\n",
      "Step 496: Loss: 13768.004 Loc: 0.855\n",
      "Step 497: Loss: 13768.004 Loc: 0.855\n",
      "Step 498: Loss: 13768.004 Loc: 0.855\n",
      "Step 499: Loss: 13768.004 Loc: 0.855\n",
      "Step 500: Loss: 13768.004 Loc: 0.855\n",
      "Step 501: Loss: 13768.004 Loc: 0.855\n",
      "Step 502: Loss: 13768.004 Loc: 0.855\n",
      "Step 503: Loss: 13768.004 Loc: 0.855\n",
      "Step 504: Loss: 13768.004 Loc: 0.855\n",
      "Step 505: Loss: 13768.004 Loc: 0.855\n",
      "Step 506: Loss: 13768.004 Loc: 0.855\n",
      "Step 507: Loss: 13768.004 Loc: 0.855\n",
      "Step 508: Loss: 13768.004 Loc: 0.855\n",
      "Step 509: Loss: 13768.004 Loc: 0.855\n",
      "Step 510: Loss: 13768.004 Loc: 0.855\n",
      "Step 511: Loss: 13768.004 Loc: 0.855\n",
      "Step 512: Loss: 13768.004 Loc: 0.855\n",
      "Step 513: Loss: 13768.004 Loc: 0.855\n",
      "Step 514: Loss: 13768.004 Loc: 0.855\n",
      "Step 515: Loss: 13768.004 Loc: 0.855\n",
      "Step 516: Loss: 13768.004 Loc: 0.855\n",
      "Step 517: Loss: 13768.004 Loc: 0.855\n",
      "Step 518: Loss: 13768.004 Loc: 0.855\n",
      "Step 519: Loss: 13768.004 Loc: 0.855\n",
      "Step 520: Loss: 13768.004 Loc: 0.855\n",
      "Step 521: Loss: 13768.004 Loc: 0.855\n",
      "Step 522: Loss: 13768.004 Loc: 0.855\n",
      "Step 523: Loss: 13768.004 Loc: 0.855\n",
      "Step 524: Loss: 13768.004 Loc: 0.855\n",
      "Step 525: Loss: 13768.004 Loc: 0.855\n",
      "Step 526: Loss: 13768.004 Loc: 0.855\n",
      "Step 527: Loss: 13768.004 Loc: 0.855\n",
      "Step 528: Loss: 13768.004 Loc: 0.855\n",
      "Step 529: Loss: 13768.004 Loc: 0.855\n",
      "Step 530: Loss: 13768.004 Loc: 0.855\n",
      "Step 531: Loss: 13768.004 Loc: 0.855\n",
      "Step 532: Loss: 13768.004 Loc: 0.855\n",
      "Step 533: Loss: 13768.004 Loc: 0.855\n",
      "Step 534: Loss: 13768.004 Loc: 0.855\n",
      "Step 535: Loss: 13768.004 Loc: 0.855\n",
      "Step 536: Loss: 13768.004 Loc: 0.855\n",
      "Step 537: Loss: 13768.004 Loc: 0.855\n",
      "Step 538: Loss: 13768.004 Loc: 0.855\n",
      "Step 539: Loss: 13768.004 Loc: 0.855\n",
      "Step 540: Loss: 13768.004 Loc: 0.855\n",
      "Step 541: Loss: 13768.004 Loc: 0.855\n",
      "Step 542: Loss: 13768.004 Loc: 0.855\n",
      "Step 543: Loss: 13768.004 Loc: 0.855\n",
      "Step 544: Loss: 13768.004 Loc: 0.855\n",
      "Step 545: Loss: 13768.004 Loc: 0.855\n",
      "Step 546: Loss: 13768.004 Loc: 0.855\n",
      "Step 547: Loss: 13768.004 Loc: 0.855\n",
      "Step 548: Loss: 13768.004 Loc: 0.855\n",
      "Step 549: Loss: 13768.004 Loc: 0.855\n",
      "Step 550: Loss: 13768.004 Loc: 0.855\n",
      "Step 551: Loss: 13768.004 Loc: 0.855\n",
      "Step 552: Loss: 13768.004 Loc: 0.855\n",
      "Step 553: Loss: 13768.004 Loc: 0.855\n",
      "Step 554: Loss: 13768.004 Loc: 0.855\n",
      "Step 555: Loss: 13768.004 Loc: 0.855\n",
      "Step 556: Loss: 13768.004 Loc: 0.855\n",
      "Step 557: Loss: 13768.004 Loc: 0.855\n",
      "Step 558: Loss: 13768.004 Loc: 0.855\n",
      "Step 559: Loss: 13768.004 Loc: 0.855\n",
      "Step 560: Loss: 13768.004 Loc: 0.855\n",
      "Step 561: Loss: 13768.004 Loc: 0.855\n",
      "Step 562: Loss: 13768.004 Loc: 0.855\n",
      "Step 563: Loss: 13768.004 Loc: 0.855\n",
      "Step 564: Loss: 13768.004 Loc: 0.855\n",
      "Step 565: Loss: 13768.004 Loc: 0.855\n",
      "Step 566: Loss: 13768.004 Loc: 0.855\n",
      "Step 567: Loss: 13768.004 Loc: 0.855\n",
      "Step 568: Loss: 13768.004 Loc: 0.855\n",
      "Step 569: Loss: 13768.004 Loc: 0.855\n",
      "Step 570: Loss: 13768.004 Loc: 0.855\n",
      "Step 571: Loss: 13768.004 Loc: 0.855\n",
      "Step 572: Loss: 13768.004 Loc: 0.855\n",
      "Step 573: Loss: 13768.004 Loc: 0.855\n",
      "Step 574: Loss: 13768.004 Loc: 0.855\n",
      "Step 575: Loss: 13768.004 Loc: 0.855\n",
      "Step 576: Loss: 13768.004 Loc: 0.855\n",
      "Step 577: Loss: 13768.004 Loc: 0.855\n",
      "Step 578: Loss: 13768.004 Loc: 0.855\n",
      "Step 579: Loss: 13768.004 Loc: 0.855\n",
      "Step 580: Loss: 13768.004 Loc: 0.855\n",
      "Step 581: Loss: 13768.004 Loc: 0.855\n",
      "Step 582: Loss: 13768.004 Loc: 0.855\n",
      "Step 583: Loss: 13768.004 Loc: 0.855\n",
      "Step 584: Loss: 13768.004 Loc: 0.855\n",
      "Step 585: Loss: 13768.004 Loc: 0.855\n",
      "Step 586: Loss: 13768.004 Loc: 0.855\n",
      "Step 587: Loss: 13768.004 Loc: 0.855\n",
      "Step 588: Loss: 13768.004 Loc: 0.855\n",
      "Step 589: Loss: 13768.004 Loc: 0.855\n",
      "Step 590: Loss: 13768.004 Loc: 0.855\n",
      "Step 591: Loss: 13768.004 Loc: 0.855\n",
      "Step 592: Loss: 13768.004 Loc: 0.855\n",
      "Step 593: Loss: 13768.004 Loc: 0.855\n",
      "Step 594: Loss: 13768.004 Loc: 0.855\n",
      "Step 595: Loss: 13768.004 Loc: 0.855\n",
      "Step 596: Loss: 13768.004 Loc: 0.855\n",
      "Step 597: Loss: 13768.004 Loc: 0.855\n",
      "Step 598: Loss: 13768.004 Loc: 0.855\n",
      "Step 599: Loss: 13768.004 Loc: 0.855\n",
      "Step 600: Loss: 13768.004 Loc: 0.855\n",
      "Step 601: Loss: 13768.004 Loc: 0.855\n",
      "Step 602: Loss: 13768.004 Loc: 0.855\n",
      "Step 603: Loss: 13768.004 Loc: 0.855\n",
      "Step 604: Loss: 13768.004 Loc: 0.855\n",
      "Step 605: Loss: 13768.004 Loc: 0.855\n",
      "Step 606: Loss: 13768.004 Loc: 0.855\n",
      "Step 607: Loss: 13768.004 Loc: 0.855\n",
      "Step 608: Loss: 13768.004 Loc: 0.855\n",
      "Step 609: Loss: 13768.004 Loc: 0.855\n",
      "Step 610: Loss: 13768.004 Loc: 0.855\n",
      "Step 611: Loss: 13768.004 Loc: 0.855\n",
      "Step 612: Loss: 13768.004 Loc: 0.855\n",
      "Step 613: Loss: 13768.004 Loc: 0.855\n",
      "Step 614: Loss: 13768.004 Loc: 0.855\n",
      "Step 615: Loss: 13768.004 Loc: 0.855\n",
      "Step 616: Loss: 13768.004 Loc: 0.855\n",
      "Step 617: Loss: 13768.004 Loc: 0.855\n",
      "Step 618: Loss: 13768.004 Loc: 0.855\n",
      "Step 619: Loss: 13768.004 Loc: 0.855\n",
      "Step 620: Loss: 13768.004 Loc: 0.855\n",
      "Step 621: Loss: 13768.004 Loc: 0.855\n",
      "Step 622: Loss: 13768.004 Loc: 0.855\n",
      "Step 623: Loss: 13768.004 Loc: 0.855\n",
      "Step 624: Loss: 13768.004 Loc: 0.855\n",
      "Step 625: Loss: 13768.004 Loc: 0.855\n",
      "Step 626: Loss: 13768.004 Loc: 0.855\n",
      "Step 627: Loss: 13768.004 Loc: 0.855\n",
      "Step 628: Loss: 13768.004 Loc: 0.855\n",
      "Step 629: Loss: 13768.004 Loc: 0.855\n",
      "Step 630: Loss: 13768.004 Loc: 0.855\n",
      "Step 631: Loss: 13768.004 Loc: 0.855\n",
      "Step 632: Loss: 13768.004 Loc: 0.855\n",
      "Step 633: Loss: 13768.004 Loc: 0.855\n",
      "Step 634: Loss: 13768.004 Loc: 0.855\n",
      "Step 635: Loss: 13768.004 Loc: 0.855\n",
      "Step 636: Loss: 13768.004 Loc: 0.855\n",
      "Step 637: Loss: 13768.004 Loc: 0.855\n",
      "Step 638: Loss: 13768.004 Loc: 0.855\n",
      "Step 639: Loss: 13768.004 Loc: 0.855\n",
      "Step 640: Loss: 13768.004 Loc: 0.855\n",
      "Step 641: Loss: 13768.004 Loc: 0.855\n",
      "Step 642: Loss: 13768.004 Loc: 0.855\n",
      "Step 643: Loss: 13768.004 Loc: 0.855\n",
      "Step 644: Loss: 13768.004 Loc: 0.855\n",
      "Step 645: Loss: 13768.004 Loc: 0.855\n",
      "Step 646: Loss: 13768.004 Loc: 0.855\n",
      "Step 647: Loss: 13768.004 Loc: 0.855\n",
      "Step 648: Loss: 13768.004 Loc: 0.855\n",
      "Step 649: Loss: 13768.004 Loc: 0.855\n",
      "Step 650: Loss: 13768.004 Loc: 0.855\n",
      "Step 651: Loss: 13768.004 Loc: 0.855\n",
      "Step 652: Loss: 13768.004 Loc: 0.855\n",
      "Step 653: Loss: 13768.004 Loc: 0.855\n",
      "Step 654: Loss: 13768.004 Loc: 0.855\n",
      "Step 655: Loss: 13768.004 Loc: 0.855\n",
      "Step 656: Loss: 13768.004 Loc: 0.855\n",
      "Step 657: Loss: 13768.004 Loc: 0.855\n",
      "Step 658: Loss: 13768.004 Loc: 0.855\n",
      "Step 659: Loss: 13768.004 Loc: 0.855\n",
      "Step 660: Loss: 13768.004 Loc: 0.855\n",
      "Step 661: Loss: 13768.004 Loc: 0.855\n",
      "Step 662: Loss: 13768.004 Loc: 0.855\n",
      "Step 663: Loss: 13768.004 Loc: 0.855\n",
      "Step 664: Loss: 13768.004 Loc: 0.855\n",
      "Step 665: Loss: 13768.004 Loc: 0.855\n",
      "Step 666: Loss: 13768.004 Loc: 0.855\n",
      "Step 667: Loss: 13768.004 Loc: 0.855\n",
      "Step 668: Loss: 13768.004 Loc: 0.855\n",
      "Step 669: Loss: 13768.004 Loc: 0.855\n",
      "Step 670: Loss: 13768.004 Loc: 0.855\n",
      "Step 671: Loss: 13768.004 Loc: 0.855\n",
      "Step 672: Loss: 13768.004 Loc: 0.855\n",
      "Step 673: Loss: 13768.004 Loc: 0.855\n",
      "Step 674: Loss: 13768.004 Loc: 0.855\n",
      "Step 675: Loss: 13768.004 Loc: 0.855\n",
      "Step 676: Loss: 13768.004 Loc: 0.855\n",
      "Step 677: Loss: 13768.004 Loc: 0.855\n",
      "Step 678: Loss: 13768.004 Loc: 0.855\n",
      "Step 679: Loss: 13768.004 Loc: 0.855\n",
      "Step 680: Loss: 13768.004 Loc: 0.855\n",
      "Step 681: Loss: 13768.004 Loc: 0.855\n",
      "Step 682: Loss: 13768.004 Loc: 0.855\n",
      "Step 683: Loss: 13768.004 Loc: 0.855\n",
      "Step 684: Loss: 13768.004 Loc: 0.855\n",
      "Step 685: Loss: 13768.004 Loc: 0.855\n",
      "Step 686: Loss: 13768.004 Loc: 0.855\n",
      "Step 687: Loss: 13768.004 Loc: 0.855\n",
      "Step 688: Loss: 13768.004 Loc: 0.855\n",
      "Step 689: Loss: 13768.004 Loc: 0.855\n",
      "Step 690: Loss: 13768.004 Loc: 0.855\n",
      "Step 691: Loss: 13768.004 Loc: 0.855\n",
      "Step 692: Loss: 13768.004 Loc: 0.855\n",
      "Step 693: Loss: 13768.004 Loc: 0.855\n",
      "Step 694: Loss: 13768.004 Loc: 0.855\n",
      "Step 695: Loss: 13768.004 Loc: 0.855\n",
      "Step 696: Loss: 13768.004 Loc: 0.855\n",
      "Step 697: Loss: 13768.004 Loc: 0.855\n",
      "Step 698: Loss: 13768.004 Loc: 0.855\n",
      "Step 699: Loss: 13768.004 Loc: 0.855\n",
      "Step 700: Loss: 13768.004 Loc: 0.855\n",
      "Step 701: Loss: 13768.004 Loc: 0.855\n",
      "Step 702: Loss: 13768.004 Loc: 0.855\n",
      "Step 703: Loss: 13768.004 Loc: 0.855\n",
      "Step 704: Loss: 13768.004 Loc: 0.855\n",
      "Step 705: Loss: 13768.004 Loc: 0.855\n",
      "Step 706: Loss: 13768.004 Loc: 0.855\n",
      "Step 707: Loss: 13768.004 Loc: 0.855\n",
      "Step 708: Loss: 13768.004 Loc: 0.855\n",
      "Step 709: Loss: 13768.004 Loc: 0.855\n",
      "Step 710: Loss: 13768.004 Loc: 0.855\n",
      "Step 711: Loss: 13768.004 Loc: 0.855\n",
      "Step 712: Loss: 13768.004 Loc: 0.855\n",
      "Step 713: Loss: 13768.004 Loc: 0.855\n",
      "Step 714: Loss: 13768.004 Loc: 0.855\n",
      "Step 715: Loss: 13768.004 Loc: 0.855\n",
      "Step 716: Loss: 13768.004 Loc: 0.855\n",
      "Step 717: Loss: 13768.004 Loc: 0.855\n",
      "Step 718: Loss: 13768.004 Loc: 0.855\n",
      "Step 719: Loss: 13768.004 Loc: 0.855\n",
      "Step 720: Loss: 13768.004 Loc: 0.855\n",
      "Step 721: Loss: 13768.004 Loc: 0.855\n",
      "Step 722: Loss: 13768.004 Loc: 0.855\n",
      "Step 723: Loss: 13768.004 Loc: 0.855\n",
      "Step 724: Loss: 13768.004 Loc: 0.855\n",
      "Step 725: Loss: 13768.004 Loc: 0.855\n",
      "Step 726: Loss: 13768.004 Loc: 0.855\n",
      "Step 727: Loss: 13768.004 Loc: 0.855\n",
      "Step 728: Loss: 13768.004 Loc: 0.855\n",
      "Step 729: Loss: 13768.004 Loc: 0.855\n",
      "Step 730: Loss: 13768.004 Loc: 0.855\n",
      "Step 731: Loss: 13768.004 Loc: 0.855\n",
      "Step 732: Loss: 13768.004 Loc: 0.855\n",
      "Step 733: Loss: 13768.004 Loc: 0.855\n",
      "Step 734: Loss: 13768.004 Loc: 0.855\n",
      "Step 735: Loss: 13768.004 Loc: 0.855\n",
      "Step 736: Loss: 13768.004 Loc: 0.855\n",
      "Step 737: Loss: 13768.004 Loc: 0.855\n",
      "Step 738: Loss: 13768.004 Loc: 0.855\n",
      "Step 739: Loss: 13768.004 Loc: 0.855\n",
      "Step 740: Loss: 13768.004 Loc: 0.855\n",
      "Step 741: Loss: 13768.004 Loc: 0.855\n",
      "Step 742: Loss: 13768.004 Loc: 0.855\n",
      "Step 743: Loss: 13768.004 Loc: 0.855\n",
      "Step 744: Loss: 13768.004 Loc: 0.855\n",
      "Step 745: Loss: 13768.004 Loc: 0.855\n",
      "Step 746: Loss: 13768.004 Loc: 0.855\n",
      "Step 747: Loss: 13768.004 Loc: 0.855\n",
      "Step 748: Loss: 13768.004 Loc: 0.855\n",
      "Step 749: Loss: 13768.004 Loc: 0.855\n",
      "Step 750: Loss: 13768.004 Loc: 0.855\n",
      "Step 751: Loss: 13768.004 Loc: 0.855\n",
      "Step 752: Loss: 13768.004 Loc: 0.855\n",
      "Step 753: Loss: 13768.004 Loc: 0.855\n",
      "Step 754: Loss: 13768.004 Loc: 0.855\n",
      "Step 755: Loss: 13768.004 Loc: 0.855\n",
      "Step 756: Loss: 13768.004 Loc: 0.855\n",
      "Step 757: Loss: 13768.004 Loc: 0.855\n",
      "Step 758: Loss: 13768.004 Loc: 0.855\n",
      "Step 759: Loss: 13768.004 Loc: 0.855\n",
      "Step 760: Loss: 13768.004 Loc: 0.855\n",
      "Step 761: Loss: 13768.004 Loc: 0.855\n",
      "Step 762: Loss: 13768.004 Loc: 0.855\n",
      "Step 763: Loss: 13768.004 Loc: 0.855\n",
      "Step 764: Loss: 13768.004 Loc: 0.855\n",
      "Step 765: Loss: 13768.004 Loc: 0.855\n",
      "Step 766: Loss: 13768.004 Loc: 0.855\n",
      "Step 767: Loss: 13768.004 Loc: 0.855\n",
      "Step 768: Loss: 13768.004 Loc: 0.855\n",
      "Step 769: Loss: 13768.004 Loc: 0.855\n",
      "Step 770: Loss: 13768.004 Loc: 0.855\n",
      "Step 771: Loss: 13768.004 Loc: 0.855\n",
      "Step 772: Loss: 13768.004 Loc: 0.855\n",
      "Step 773: Loss: 13768.004 Loc: 0.855\n",
      "Step 774: Loss: 13768.004 Loc: 0.855\n",
      "Step 775: Loss: 13768.004 Loc: 0.855\n",
      "Step 776: Loss: 13768.004 Loc: 0.855\n",
      "Step 777: Loss: 13768.004 Loc: 0.855\n",
      "Step 778: Loss: 13768.004 Loc: 0.855\n",
      "Step 779: Loss: 13768.004 Loc: 0.855\n",
      "Step 780: Loss: 13768.004 Loc: 0.855\n",
      "Step 781: Loss: 13768.004 Loc: 0.855\n",
      "Step 782: Loss: 13768.004 Loc: 0.855\n",
      "Step 783: Loss: 13768.004 Loc: 0.855\n",
      "Step 784: Loss: 13768.004 Loc: 0.855\n",
      "Step 785: Loss: 13768.004 Loc: 0.855\n",
      "Step 786: Loss: 13768.004 Loc: 0.855\n",
      "Step 787: Loss: 13768.004 Loc: 0.855\n",
      "Step 788: Loss: 13768.004 Loc: 0.855\n",
      "Step 789: Loss: 13768.004 Loc: 0.855\n",
      "Step 790: Loss: 13768.004 Loc: 0.855\n",
      "Step 791: Loss: 13768.004 Loc: 0.855\n",
      "Step 792: Loss: 13768.004 Loc: 0.855\n",
      "Step 793: Loss: 13768.004 Loc: 0.855\n",
      "Step 794: Loss: 13768.004 Loc: 0.855\n",
      "Step 795: Loss: 13768.004 Loc: 0.855\n",
      "Step 796: Loss: 13768.004 Loc: 0.855\n",
      "Step 797: Loss: 13768.004 Loc: 0.855\n",
      "Step 798: Loss: 13768.004 Loc: 0.855\n",
      "Step 799: Loss: 13768.004 Loc: 0.855\n",
      "Step 800: Loss: 13768.004 Loc: 0.855\n",
      "Step 801: Loss: 13768.004 Loc: 0.855\n",
      "Step 802: Loss: 13768.004 Loc: 0.855\n",
      "Step 803: Loss: 13768.004 Loc: 0.855\n",
      "Step 804: Loss: 13768.004 Loc: 0.855\n",
      "Step 805: Loss: 13768.004 Loc: 0.855\n",
      "Step 806: Loss: 13768.004 Loc: 0.855\n",
      "Step 807: Loss: 13768.004 Loc: 0.855\n",
      "Step 808: Loss: 13768.004 Loc: 0.855\n",
      "Step 809: Loss: 13768.004 Loc: 0.855\n",
      "Step 810: Loss: 13768.004 Loc: 0.855\n",
      "Step 811: Loss: 13768.004 Loc: 0.855\n",
      "Step 812: Loss: 13768.004 Loc: 0.855\n",
      "Step 813: Loss: 13768.004 Loc: 0.855\n",
      "Step 814: Loss: 13768.004 Loc: 0.855\n",
      "Step 815: Loss: 13768.004 Loc: 0.855\n",
      "Step 816: Loss: 13768.004 Loc: 0.855\n",
      "Step 817: Loss: 13768.004 Loc: 0.855\n",
      "Step 818: Loss: 13768.004 Loc: 0.855\n",
      "Step 819: Loss: 13768.004 Loc: 0.855\n",
      "Step 820: Loss: 13768.004 Loc: 0.855\n",
      "Step 821: Loss: 13768.004 Loc: 0.855\n",
      "Step 822: Loss: 13768.004 Loc: 0.855\n",
      "Step 823: Loss: 13768.004 Loc: 0.855\n",
      "Step 824: Loss: 13768.004 Loc: 0.855\n",
      "Step 825: Loss: 13768.004 Loc: 0.855\n",
      "Step 826: Loss: 13768.004 Loc: 0.855\n",
      "Step 827: Loss: 13768.004 Loc: 0.855\n",
      "Step 828: Loss: 13768.004 Loc: 0.855\n",
      "Step 829: Loss: 13768.004 Loc: 0.855\n",
      "Step 830: Loss: 13768.004 Loc: 0.855\n",
      "Step 831: Loss: 13768.004 Loc: 0.855\n",
      "Step 832: Loss: 13768.004 Loc: 0.855\n",
      "Step 833: Loss: 13768.004 Loc: 0.855\n",
      "Step 834: Loss: 13768.004 Loc: 0.855\n",
      "Step 835: Loss: 13768.004 Loc: 0.855\n",
      "Step 836: Loss: 13768.004 Loc: 0.855\n",
      "Step 837: Loss: 13768.004 Loc: 0.855\n",
      "Step 838: Loss: 13768.004 Loc: 0.855\n",
      "Step 839: Loss: 13768.004 Loc: 0.855\n",
      "Step 840: Loss: 13768.004 Loc: 0.855\n",
      "Step 841: Loss: 13768.004 Loc: 0.855\n",
      "Step 842: Loss: 13768.004 Loc: 0.855\n",
      "Step 843: Loss: 13768.004 Loc: 0.855\n",
      "Step 844: Loss: 13768.004 Loc: 0.855\n",
      "Step 845: Loss: 13768.004 Loc: 0.855\n",
      "Step 846: Loss: 13768.004 Loc: 0.855\n",
      "Step 847: Loss: 13768.004 Loc: 0.855\n",
      "Step 848: Loss: 13768.004 Loc: 0.855\n",
      "Step 849: Loss: 13768.004 Loc: 0.855\n",
      "Step 850: Loss: 13768.004 Loc: 0.855\n",
      "Step 851: Loss: 13768.004 Loc: 0.855\n",
      "Step 852: Loss: 13768.004 Loc: 0.855\n",
      "Step 853: Loss: 13768.004 Loc: 0.855\n",
      "Step 854: Loss: 13768.004 Loc: 0.855\n",
      "Step 855: Loss: 13768.004 Loc: 0.855\n",
      "Step 856: Loss: 13768.004 Loc: 0.855\n",
      "Step 857: Loss: 13768.004 Loc: 0.855\n",
      "Step 858: Loss: 13768.004 Loc: 0.855\n",
      "Step 859: Loss: 13768.004 Loc: 0.855\n",
      "Step 860: Loss: 13768.004 Loc: 0.855\n",
      "Step 861: Loss: 13768.004 Loc: 0.855\n",
      "Step 862: Loss: 13768.004 Loc: 0.855\n",
      "Step 863: Loss: 13768.004 Loc: 0.855\n",
      "Step 864: Loss: 13768.004 Loc: 0.855\n",
      "Step 865: Loss: 13768.004 Loc: 0.855\n",
      "Step 866: Loss: 13768.004 Loc: 0.855\n",
      "Step 867: Loss: 13768.004 Loc: 0.855\n",
      "Step 868: Loss: 13768.004 Loc: 0.855\n",
      "Step 869: Loss: 13768.004 Loc: 0.855\n",
      "Step 870: Loss: 13768.004 Loc: 0.855\n",
      "Step 871: Loss: 13768.004 Loc: 0.855\n",
      "Step 872: Loss: 13768.004 Loc: 0.855\n",
      "Step 873: Loss: 13768.004 Loc: 0.855\n",
      "Step 874: Loss: 13768.004 Loc: 0.855\n",
      "Step 875: Loss: 13768.004 Loc: 0.855\n",
      "Step 876: Loss: 13768.004 Loc: 0.855\n",
      "Step 877: Loss: 13768.004 Loc: 0.855\n",
      "Step 878: Loss: 13768.004 Loc: 0.855\n",
      "Step 879: Loss: 13768.004 Loc: 0.855\n",
      "Step 880: Loss: 13768.004 Loc: 0.855\n",
      "Step 881: Loss: 13768.004 Loc: 0.855\n",
      "Step 882: Loss: 13768.004 Loc: 0.855\n",
      "Step 883: Loss: 13768.004 Loc: 0.855\n",
      "Step 884: Loss: 13768.004 Loc: 0.855\n",
      "Step 885: Loss: 13768.004 Loc: 0.855\n",
      "Step 886: Loss: 13768.004 Loc: 0.855\n",
      "Step 887: Loss: 13768.004 Loc: 0.855\n",
      "Step 888: Loss: 13768.004 Loc: 0.855\n",
      "Step 889: Loss: 13768.004 Loc: 0.855\n",
      "Step 890: Loss: 13768.004 Loc: 0.855\n",
      "Step 891: Loss: 13768.004 Loc: 0.855\n",
      "Step 892: Loss: 13768.004 Loc: 0.855\n",
      "Step 893: Loss: 13768.004 Loc: 0.855\n",
      "Step 894: Loss: 13768.004 Loc: 0.855\n",
      "Step 895: Loss: 13768.004 Loc: 0.855\n",
      "Step 896: Loss: 13768.004 Loc: 0.855\n",
      "Step 897: Loss: 13768.004 Loc: 0.855\n",
      "Step 898: Loss: 13768.004 Loc: 0.855\n",
      "Step 899: Loss: 13768.004 Loc: 0.855\n",
      "Step 900: Loss: 13768.004 Loc: 0.855\n",
      "Step 901: Loss: 13768.004 Loc: 0.855\n",
      "Step 902: Loss: 13768.004 Loc: 0.855\n",
      "Step 903: Loss: 13768.004 Loc: 0.855\n",
      "Step 904: Loss: 13768.004 Loc: 0.855\n",
      "Step 905: Loss: 13768.004 Loc: 0.855\n",
      "Step 906: Loss: 13768.004 Loc: 0.855\n",
      "Step 907: Loss: 13768.004 Loc: 0.855\n",
      "Step 908: Loss: 13768.004 Loc: 0.855\n",
      "Step 909: Loss: 13768.004 Loc: 0.855\n",
      "Step 910: Loss: 13768.004 Loc: 0.855\n",
      "Step 911: Loss: 13768.004 Loc: 0.855\n",
      "Step 912: Loss: 13768.004 Loc: 0.855\n",
      "Step 913: Loss: 13768.004 Loc: 0.855\n",
      "Step 914: Loss: 13768.004 Loc: 0.855\n",
      "Step 915: Loss: 13768.004 Loc: 0.855\n",
      "Step 916: Loss: 13768.004 Loc: 0.855\n",
      "Step 917: Loss: 13768.004 Loc: 0.855\n",
      "Step 918: Loss: 13768.004 Loc: 0.855\n",
      "Step 919: Loss: 13768.004 Loc: 0.855\n",
      "Step 920: Loss: 13768.004 Loc: 0.855\n",
      "Step 921: Loss: 13768.004 Loc: 0.855\n",
      "Step 922: Loss: 13768.004 Loc: 0.855\n",
      "Step 923: Loss: 13768.004 Loc: 0.855\n",
      "Step 924: Loss: 13768.004 Loc: 0.855\n",
      "Step 925: Loss: 13768.004 Loc: 0.855\n",
      "Step 926: Loss: 13768.004 Loc: 0.855\n",
      "Step 927: Loss: 13768.004 Loc: 0.855\n",
      "Step 928: Loss: 13768.004 Loc: 0.855\n",
      "Step 929: Loss: 13768.004 Loc: 0.855\n",
      "Step 930: Loss: 13768.004 Loc: 0.855\n",
      "Step 931: Loss: 13768.004 Loc: 0.855\n",
      "Step 932: Loss: 13768.004 Loc: 0.855\n",
      "Step 933: Loss: 13768.004 Loc: 0.855\n",
      "Step 934: Loss: 13768.004 Loc: 0.855\n",
      "Step 935: Loss: 13768.004 Loc: 0.855\n",
      "Step 936: Loss: 13768.004 Loc: 0.855\n",
      "Step 937: Loss: 13768.004 Loc: 0.855\n",
      "Step 938: Loss: 13768.004 Loc: 0.855\n",
      "Step 939: Loss: 13768.004 Loc: 0.855\n",
      "Step 940: Loss: 13768.004 Loc: 0.855\n",
      "Step 941: Loss: 13768.004 Loc: 0.855\n",
      "Step 942: Loss: 13768.004 Loc: 0.855\n",
      "Step 943: Loss: 13768.004 Loc: 0.855\n",
      "Step 944: Loss: 13768.004 Loc: 0.855\n",
      "Step 945: Loss: 13768.004 Loc: 0.855\n",
      "Step 946: Loss: 13768.004 Loc: 0.855\n",
      "Step 947: Loss: 13768.004 Loc: 0.855\n",
      "Step 948: Loss: 13768.004 Loc: 0.855\n",
      "Step 949: Loss: 13768.004 Loc: 0.855\n",
      "Step 950: Loss: 13768.004 Loc: 0.855\n",
      "Step 951: Loss: 13768.004 Loc: 0.855\n",
      "Step 952: Loss: 13768.004 Loc: 0.855\n",
      "Step 953: Loss: 13768.004 Loc: 0.855\n",
      "Step 954: Loss: 13768.004 Loc: 0.855\n",
      "Step 955: Loss: 13768.004 Loc: 0.855\n",
      "Step 956: Loss: 13768.004 Loc: 0.855\n",
      "Step 957: Loss: 13768.004 Loc: 0.855\n",
      "Step 958: Loss: 13768.004 Loc: 0.855\n",
      "Step 959: Loss: 13768.004 Loc: 0.855\n",
      "Step 960: Loss: 13768.004 Loc: 0.855\n",
      "Step 961: Loss: 13768.004 Loc: 0.855\n",
      "Step 962: Loss: 13768.004 Loc: 0.855\n",
      "Step 963: Loss: 13768.004 Loc: 0.855\n",
      "Step 964: Loss: 13768.004 Loc: 0.855\n",
      "Step 965: Loss: 13768.004 Loc: 0.855\n",
      "Step 966: Loss: 13768.004 Loc: 0.855\n",
      "Step 967: Loss: 13768.004 Loc: 0.855\n",
      "Step 968: Loss: 13768.004 Loc: 0.855\n",
      "Step 969: Loss: 13768.004 Loc: 0.855\n",
      "Step 970: Loss: 13768.004 Loc: 0.855\n",
      "Step 971: Loss: 13768.004 Loc: 0.855\n",
      "Step 972: Loss: 13768.004 Loc: 0.855\n",
      "Step 973: Loss: 13768.004 Loc: 0.855\n",
      "Step 974: Loss: 13768.004 Loc: 0.855\n",
      "Step 975: Loss: 13768.004 Loc: 0.855\n",
      "Step 976: Loss: 13768.004 Loc: 0.855\n",
      "Step 977: Loss: 13768.004 Loc: 0.855\n",
      "Step 978: Loss: 13768.004 Loc: 0.855\n",
      "Step 979: Loss: 13768.004 Loc: 0.855\n",
      "Step 980: Loss: 13768.004 Loc: 0.855\n",
      "Step 981: Loss: 13768.004 Loc: 0.855\n",
      "Step 982: Loss: 13768.004 Loc: 0.855\n",
      "Step 983: Loss: 13768.004 Loc: 0.855\n",
      "Step 984: Loss: 13768.004 Loc: 0.855\n",
      "Step 985: Loss: 13768.004 Loc: 0.855\n",
      "Step 986: Loss: 13768.004 Loc: 0.855\n",
      "Step 987: Loss: 13768.004 Loc: 0.855\n",
      "Step 988: Loss: 13768.004 Loc: 0.855\n",
      "Step 989: Loss: 13768.004 Loc: 0.855\n",
      "Step 990: Loss: 13768.004 Loc: 0.855\n",
      "Step 991: Loss: 13768.004 Loc: 0.855\n",
      "Step 992: Loss: 13768.004 Loc: 0.855\n",
      "Step 993: Loss: 13768.004 Loc: 0.855\n",
      "Step 994: Loss: 13768.004 Loc: 0.855\n",
      "Step 995: Loss: 13768.004 Loc: 0.855\n",
      "Step 996: Loss: 13768.004 Loc: 0.855\n",
      "Step 997: Loss: 13768.004 Loc: 0.855\n",
      "Step 998: Loss: 13768.004 Loc: 0.855\n",
      "Step 999: Loss: 13768.004 Loc: 0.855\n",
      "Step 1000: Loss: 13768.004 Loc: 0.855\n",
      "Step 1001: Loss: 13768.004 Loc: 0.855\n",
      "Step 1002: Loss: 13768.004 Loc: 0.855\n",
      "Step 1003: Loss: 13768.004 Loc: 0.855\n",
      "Step 1004: Loss: 13768.004 Loc: 0.855\n",
      "Step 1005: Loss: 13768.004 Loc: 0.855\n",
      "Step 1006: Loss: 13768.004 Loc: 0.855\n",
      "Step 1007: Loss: 13768.004 Loc: 0.855\n",
      "Step 1008: Loss: 13768.004 Loc: 0.855\n",
      "Step 1009: Loss: 13768.004 Loc: 0.855\n",
      "Step 1010: Loss: 13768.004 Loc: 0.855\n",
      "Step 1011: Loss: 13768.004 Loc: 0.855\n",
      "Step 1012: Loss: 13768.004 Loc: 0.855\n",
      "Step 1013: Loss: 13768.004 Loc: 0.855\n",
      "Step 1014: Loss: 13768.004 Loc: 0.855\n",
      "Step 1015: Loss: 13768.004 Loc: 0.855\n",
      "Step 1016: Loss: 13768.004 Loc: 0.855\n",
      "Step 1017: Loss: 13768.004 Loc: 0.855\n",
      "Step 1018: Loss: 13768.004 Loc: 0.855\n",
      "Step 1019: Loss: 13768.004 Loc: 0.855\n",
      "Step 1020: Loss: 13768.004 Loc: 0.855\n",
      "Step 1021: Loss: 13768.004 Loc: 0.855\n",
      "Step 1022: Loss: 13768.004 Loc: 0.855\n",
      "Step 1023: Loss: 13768.004 Loc: 0.855\n",
      "Step 1024: Loss: 13768.004 Loc: 0.855\n",
      "Step 1025: Loss: 13768.004 Loc: 0.855\n",
      "Step 1026: Loss: 13768.004 Loc: 0.855\n",
      "Step 1027: Loss: 13768.004 Loc: 0.855\n",
      "Step 1028: Loss: 13768.004 Loc: 0.855\n",
      "Step 1029: Loss: 13768.004 Loc: 0.855\n",
      "Step 1030: Loss: 13768.004 Loc: 0.855\n",
      "Step 1031: Loss: 13768.004 Loc: 0.855\n",
      "Step 1032: Loss: 13768.004 Loc: 0.855\n",
      "Step 1033: Loss: 13768.004 Loc: 0.855\n",
      "Step 1034: Loss: 13768.004 Loc: 0.855\n",
      "Step 1035: Loss: 13768.004 Loc: 0.855\n",
      "Step 1036: Loss: 13768.004 Loc: 0.855\n",
      "Step 1037: Loss: 13768.004 Loc: 0.855\n",
      "Step 1038: Loss: 13768.004 Loc: 0.855\n",
      "Step 1039: Loss: 13768.004 Loc: 0.855\n",
      "Step 1040: Loss: 13768.004 Loc: 0.855\n",
      "Step 1041: Loss: 13768.004 Loc: 0.855\n",
      "Step 1042: Loss: 13768.004 Loc: 0.855\n",
      "Step 1043: Loss: 13768.004 Loc: 0.855\n",
      "Step 1044: Loss: 13768.004 Loc: 0.855\n",
      "Step 1045: Loss: 13768.004 Loc: 0.855\n",
      "Step 1046: Loss: 13768.004 Loc: 0.855\n",
      "Step 1047: Loss: 13768.004 Loc: 0.855\n",
      "Step 1048: Loss: 13768.004 Loc: 0.855\n",
      "Step 1049: Loss: 13768.004 Loc: 0.855\n",
      "Step 1050: Loss: 13768.004 Loc: 0.855\n",
      "Step 1051: Loss: 13768.004 Loc: 0.855\n",
      "Step 1052: Loss: 13768.004 Loc: 0.855\n",
      "Step 1053: Loss: 13768.004 Loc: 0.855\n",
      "Step 1054: Loss: 13768.004 Loc: 0.855\n",
      "Step 1055: Loss: 13768.004 Loc: 0.855\n",
      "Step 1056: Loss: 13768.004 Loc: 0.855\n",
      "Step 1057: Loss: 13768.004 Loc: 0.855\n",
      "Step 1058: Loss: 13768.004 Loc: 0.855\n",
      "Step 1059: Loss: 13768.004 Loc: 0.855\n",
      "Step 1060: Loss: 13768.004 Loc: 0.855\n",
      "Step 1061: Loss: 13768.004 Loc: 0.855\n",
      "Step 1062: Loss: 13768.004 Loc: 0.855\n",
      "Step 1063: Loss: 13768.004 Loc: 0.855\n",
      "Step 1064: Loss: 13768.004 Loc: 0.855\n",
      "Step 1065: Loss: 13768.004 Loc: 0.855\n",
      "Step 1066: Loss: 13768.004 Loc: 0.855\n",
      "Step 1067: Loss: 13768.004 Loc: 0.855\n",
      "Step 1068: Loss: 13768.004 Loc: 0.855\n",
      "Step 1069: Loss: 13768.004 Loc: 0.855\n",
      "Step 1070: Loss: 13768.004 Loc: 0.855\n",
      "Step 1071: Loss: 13768.004 Loc: 0.855\n",
      "Step 1072: Loss: 13768.004 Loc: 0.855\n",
      "Step 1073: Loss: 13768.004 Loc: 0.855\n",
      "Step 1074: Loss: 13768.004 Loc: 0.855\n",
      "Step 1075: Loss: 13768.004 Loc: 0.855\n",
      "Step 1076: Loss: 13768.004 Loc: 0.855\n",
      "Step 1077: Loss: 13768.004 Loc: 0.855\n",
      "Step 1078: Loss: 13768.004 Loc: 0.855\n",
      "Step 1079: Loss: 13768.004 Loc: 0.855\n",
      "Step 1080: Loss: 13768.004 Loc: 0.855\n",
      "Step 1081: Loss: 13768.004 Loc: 0.855\n",
      "Step 1082: Loss: 13768.004 Loc: 0.855\n",
      "Step 1083: Loss: 13768.004 Loc: 0.855\n",
      "Step 1084: Loss: 13768.004 Loc: 0.855\n",
      "Step 1085: Loss: 13768.004 Loc: 0.855\n",
      "Step 1086: Loss: 13768.004 Loc: 0.855\n",
      "Step 1087: Loss: 13768.004 Loc: 0.855\n",
      "Step 1088: Loss: 13768.004 Loc: 0.855\n",
      "Step 1089: Loss: 13768.004 Loc: 0.855\n",
      "Step 1090: Loss: 13768.004 Loc: 0.855\n",
      "Step 1091: Loss: 13768.004 Loc: 0.855\n",
      "Step 1092: Loss: 13768.004 Loc: 0.855\n",
      "Step 1093: Loss: 13768.004 Loc: 0.855\n",
      "Step 1094: Loss: 13768.004 Loc: 0.855\n",
      "Step 1095: Loss: 13768.004 Loc: 0.855\n",
      "Step 1096: Loss: 13768.004 Loc: 0.855\n",
      "Step 1097: Loss: 13768.004 Loc: 0.855\n",
      "Step 1098: Loss: 13768.004 Loc: 0.855\n",
      "Step 1099: Loss: 13768.004 Loc: 0.855\n",
      "Step 1100: Loss: 13768.004 Loc: 0.855\n",
      "Step 1101: Loss: 13768.004 Loc: 0.855\n",
      "Step 1102: Loss: 13768.004 Loc: 0.855\n",
      "Step 1103: Loss: 13768.004 Loc: 0.855\n",
      "Step 1104: Loss: 13768.004 Loc: 0.855\n",
      "Step 1105: Loss: 13768.004 Loc: 0.855\n",
      "Step 1106: Loss: 13768.004 Loc: 0.855\n",
      "Step 1107: Loss: 13768.004 Loc: 0.855\n",
      "Step 1108: Loss: 13768.004 Loc: 0.855\n",
      "Step 1109: Loss: 13768.004 Loc: 0.855\n",
      "Step 1110: Loss: 13768.004 Loc: 0.855\n",
      "Step 1111: Loss: 13768.004 Loc: 0.855\n",
      "Step 1112: Loss: 13768.004 Loc: 0.855\n",
      "Step 1113: Loss: 13768.004 Loc: 0.855\n",
      "Step 1114: Loss: 13768.004 Loc: 0.855\n",
      "Step 1115: Loss: 13768.004 Loc: 0.855\n",
      "Step 1116: Loss: 13768.004 Loc: 0.855\n",
      "Step 1117: Loss: 13768.004 Loc: 0.855\n",
      "Step 1118: Loss: 13768.004 Loc: 0.855\n",
      "Step 1119: Loss: 13768.004 Loc: 0.855\n",
      "Step 1120: Loss: 13768.004 Loc: 0.855\n",
      "Step 1121: Loss: 13768.004 Loc: 0.855\n",
      "Step 1122: Loss: 13768.004 Loc: 0.855\n",
      "Step 1123: Loss: 13768.004 Loc: 0.855\n",
      "Step 1124: Loss: 13768.004 Loc: 0.855\n",
      "Step 1125: Loss: 13768.004 Loc: 0.855\n",
      "Step 1126: Loss: 13768.004 Loc: 0.855\n",
      "Step 1127: Loss: 13768.004 Loc: 0.855\n",
      "Step 1128: Loss: 13768.004 Loc: 0.855\n",
      "Step 1129: Loss: 13768.004 Loc: 0.855\n",
      "Step 1130: Loss: 13768.004 Loc: 0.855\n",
      "Step 1131: Loss: 13768.004 Loc: 0.855\n",
      "Step 1132: Loss: 13768.004 Loc: 0.855\n",
      "Step 1133: Loss: 13768.004 Loc: 0.855\n",
      "Step 1134: Loss: 13768.004 Loc: 0.855\n",
      "Step 1135: Loss: 13768.004 Loc: 0.855\n",
      "Step 1136: Loss: 13768.004 Loc: 0.855\n",
      "Step 1137: Loss: 13768.004 Loc: 0.855\n",
      "Step 1138: Loss: 13768.004 Loc: 0.855\n",
      "Step 1139: Loss: 13768.004 Loc: 0.855\n",
      "Step 1140: Loss: 13768.004 Loc: 0.855\n",
      "Step 1141: Loss: 13768.004 Loc: 0.855\n",
      "Step 1142: Loss: 13768.004 Loc: 0.855\n",
      "Step 1143: Loss: 13768.004 Loc: 0.855\n",
      "Step 1144: Loss: 13768.004 Loc: 0.855\n",
      "Step 1145: Loss: 13768.004 Loc: 0.855\n",
      "Step 1146: Loss: 13768.004 Loc: 0.855\n",
      "Step 1147: Loss: 13768.004 Loc: 0.855\n",
      "Step 1148: Loss: 13768.004 Loc: 0.855\n",
      "Step 1149: Loss: 13768.004 Loc: 0.855\n",
      "Step 1150: Loss: 13768.004 Loc: 0.855\n",
      "Step 1151: Loss: 13768.004 Loc: 0.855\n",
      "Step 1152: Loss: 13768.004 Loc: 0.855\n",
      "Step 1153: Loss: 13768.004 Loc: 0.855\n",
      "Step 1154: Loss: 13768.004 Loc: 0.855\n",
      "Step 1155: Loss: 13768.004 Loc: 0.855\n",
      "Step 1156: Loss: 13768.004 Loc: 0.855\n",
      "Step 1157: Loss: 13768.004 Loc: 0.855\n",
      "Step 1158: Loss: 13768.004 Loc: 0.855\n",
      "Step 1159: Loss: 13768.004 Loc: 0.855\n",
      "Step 1160: Loss: 13768.004 Loc: 0.855\n",
      "Step 1161: Loss: 13768.004 Loc: 0.855\n",
      "Step 1162: Loss: 13768.004 Loc: 0.855\n",
      "Step 1163: Loss: 13768.004 Loc: 0.855\n",
      "Step 1164: Loss: 13768.004 Loc: 0.855\n",
      "Step 1165: Loss: 13768.004 Loc: 0.855\n",
      "Step 1166: Loss: 13768.004 Loc: 0.855\n",
      "Step 1167: Loss: 13768.004 Loc: 0.855\n",
      "Step 1168: Loss: 13768.004 Loc: 0.855\n",
      "Step 1169: Loss: 13768.004 Loc: 0.855\n",
      "Step 1170: Loss: 13768.004 Loc: 0.855\n",
      "Step 1171: Loss: 13768.004 Loc: 0.855\n",
      "Step 1172: Loss: 13768.004 Loc: 0.855\n",
      "Step 1173: Loss: 13768.004 Loc: 0.855\n",
      "Step 1174: Loss: 13768.004 Loc: 0.855\n",
      "Step 1175: Loss: 13768.004 Loc: 0.855\n",
      "Step 1176: Loss: 13768.004 Loc: 0.855\n",
      "Step 1177: Loss: 13768.004 Loc: 0.855\n",
      "Step 1178: Loss: 13768.004 Loc: 0.855\n",
      "Step 1179: Loss: 13768.004 Loc: 0.855\n",
      "Step 1180: Loss: 13768.004 Loc: 0.855\n",
      "Step 1181: Loss: 13768.004 Loc: 0.855\n",
      "Step 1182: Loss: 13768.004 Loc: 0.855\n",
      "Step 1183: Loss: 13768.004 Loc: 0.855\n",
      "Step 1184: Loss: 13768.004 Loc: 0.855\n",
      "Step 1185: Loss: 13768.004 Loc: 0.855\n",
      "Step 1186: Loss: 13768.004 Loc: 0.855\n",
      "Step 1187: Loss: 13768.004 Loc: 0.855\n",
      "Step 1188: Loss: 13768.004 Loc: 0.855\n",
      "Step 1189: Loss: 13768.004 Loc: 0.855\n",
      "Step 1190: Loss: 13768.004 Loc: 0.855\n",
      "Step 1191: Loss: 13768.004 Loc: 0.855\n",
      "Step 1192: Loss: 13768.004 Loc: 0.855\n",
      "Step 1193: Loss: 13768.004 Loc: 0.855\n",
      "Step 1194: Loss: 13768.004 Loc: 0.855\n",
      "Step 1195: Loss: 13768.004 Loc: 0.855\n",
      "Step 1196: Loss: 13768.004 Loc: 0.855\n",
      "Step 1197: Loss: 13768.004 Loc: 0.855\n",
      "Step 1198: Loss: 13768.004 Loc: 0.855\n",
      "Step 1199: Loss: 13768.004 Loc: 0.855\n",
      "Step 1200: Loss: 13768.004 Loc: 0.855\n",
      "Step 1201: Loss: 13768.004 Loc: 0.855\n",
      "Step 1202: Loss: 13768.004 Loc: 0.855\n",
      "Step 1203: Loss: 13768.004 Loc: 0.855\n",
      "Step 1204: Loss: 13768.004 Loc: 0.855\n",
      "Step 1205: Loss: 13768.004 Loc: 0.855\n",
      "Step 1206: Loss: 13768.004 Loc: 0.855\n",
      "Step 1207: Loss: 13768.004 Loc: 0.855\n",
      "Step 1208: Loss: 13768.004 Loc: 0.855\n",
      "Step 1209: Loss: 13768.004 Loc: 0.855\n",
      "Step 1210: Loss: 13768.004 Loc: 0.855\n",
      "Step 1211: Loss: 13768.004 Loc: 0.855\n",
      "Step 1212: Loss: 13768.004 Loc: 0.855\n",
      "Step 1213: Loss: 13768.004 Loc: 0.855\n",
      "Step 1214: Loss: 13768.004 Loc: 0.855\n",
      "Step 1215: Loss: 13768.004 Loc: 0.855\n",
      "Step 1216: Loss: 13768.004 Loc: 0.855\n",
      "Step 1217: Loss: 13768.004 Loc: 0.855\n",
      "Step 1218: Loss: 13768.004 Loc: 0.855\n",
      "Step 1219: Loss: 13768.004 Loc: 0.855\n",
      "Step 1220: Loss: 13768.004 Loc: 0.855\n",
      "Step 1221: Loss: 13768.004 Loc: 0.855\n",
      "Step 1222: Loss: 13768.004 Loc: 0.855\n",
      "Step 1223: Loss: 13768.004 Loc: 0.855\n",
      "Step 1224: Loss: 13768.004 Loc: 0.855\n",
      "Step 1225: Loss: 13768.004 Loc: 0.855\n",
      "Step 1226: Loss: 13768.004 Loc: 0.855\n",
      "Step 1227: Loss: 13768.004 Loc: 0.855\n",
      "Step 1228: Loss: 13768.004 Loc: 0.855\n",
      "Step 1229: Loss: 13768.004 Loc: 0.855\n",
      "Step 1230: Loss: 13768.004 Loc: 0.855\n",
      "Step 1231: Loss: 13768.004 Loc: 0.855\n",
      "Step 1232: Loss: 13768.004 Loc: 0.855\n",
      "Step 1233: Loss: 13768.004 Loc: 0.855\n",
      "Step 1234: Loss: 13768.004 Loc: 0.855\n",
      "Step 1235: Loss: 13768.004 Loc: 0.855\n",
      "Step 1236: Loss: 13768.004 Loc: 0.855\n",
      "Step 1237: Loss: 13768.004 Loc: 0.855\n",
      "Step 1238: Loss: 13768.004 Loc: 0.855\n",
      "Step 1239: Loss: 13768.004 Loc: 0.855\n",
      "Step 1240: Loss: 13768.004 Loc: 0.855\n",
      "Step 1241: Loss: 13768.004 Loc: 0.855\n",
      "Step 1242: Loss: 13768.004 Loc: 0.855\n",
      "Step 1243: Loss: 13768.004 Loc: 0.855\n",
      "Step 1244: Loss: 13768.004 Loc: 0.855\n",
      "Step 1245: Loss: 13768.004 Loc: 0.855\n",
      "Step 1246: Loss: 13768.004 Loc: 0.855\n",
      "Step 1247: Loss: 13768.004 Loc: 0.855\n",
      "Step 1248: Loss: 13768.004 Loc: 0.855\n",
      "Step 1249: Loss: 13768.004 Loc: 0.855\n",
      "Step 1250: Loss: 13768.004 Loc: 0.855\n",
      "Step 1251: Loss: 13768.004 Loc: 0.855\n",
      "Step 1252: Loss: 13768.004 Loc: 0.855\n",
      "Step 1253: Loss: 13768.004 Loc: 0.855\n",
      "Step 1254: Loss: 13768.004 Loc: 0.855\n",
      "Step 1255: Loss: 13768.004 Loc: 0.855\n",
      "Step 1256: Loss: 13768.004 Loc: 0.855\n",
      "Step 1257: Loss: 13768.004 Loc: 0.855\n",
      "Step 1258: Loss: 13768.004 Loc: 0.855\n",
      "Step 1259: Loss: 13768.004 Loc: 0.855\n",
      "Step 1260: Loss: 13768.004 Loc: 0.855\n",
      "Step 1261: Loss: 13768.004 Loc: 0.855\n",
      "Step 1262: Loss: 13768.004 Loc: 0.855\n",
      "Step 1263: Loss: 13768.004 Loc: 0.855\n",
      "Step 1264: Loss: 13768.004 Loc: 0.855\n",
      "Step 1265: Loss: 13768.004 Loc: 0.855\n",
      "Step 1266: Loss: 13768.004 Loc: 0.855\n",
      "Step 1267: Loss: 13768.004 Loc: 0.855\n",
      "Step 1268: Loss: 13768.004 Loc: 0.855\n",
      "Step 1269: Loss: 13768.004 Loc: 0.855\n",
      "Step 1270: Loss: 13768.004 Loc: 0.855\n",
      "Step 1271: Loss: 13768.004 Loc: 0.855\n",
      "Step 1272: Loss: 13768.004 Loc: 0.855\n",
      "Step 1273: Loss: 13768.004 Loc: 0.855\n",
      "Step 1274: Loss: 13768.004 Loc: 0.855\n",
      "Step 1275: Loss: 13768.004 Loc: 0.855\n",
      "Step 1276: Loss: 13768.004 Loc: 0.855\n",
      "Step 1277: Loss: 13768.004 Loc: 0.855\n",
      "Step 1278: Loss: 13768.004 Loc: 0.855\n",
      "Step 1279: Loss: 13768.004 Loc: 0.855\n",
      "Step 1280: Loss: 13768.004 Loc: 0.855\n",
      "Step 1281: Loss: 13768.004 Loc: 0.855\n",
      "Step 1282: Loss: 13768.004 Loc: 0.855\n",
      "Step 1283: Loss: 13768.004 Loc: 0.855\n",
      "Step 1284: Loss: 13768.004 Loc: 0.855\n",
      "Step 1285: Loss: 13768.004 Loc: 0.855\n",
      "Step 1286: Loss: 13768.004 Loc: 0.855\n",
      "Step 1287: Loss: 13768.004 Loc: 0.855\n",
      "Step 1288: Loss: 13768.004 Loc: 0.855\n",
      "Step 1289: Loss: 13768.004 Loc: 0.855\n",
      "Step 1290: Loss: 13768.004 Loc: 0.855\n",
      "Step 1291: Loss: 13768.004 Loc: 0.855\n",
      "Step 1292: Loss: 13768.004 Loc: 0.855\n",
      "Step 1293: Loss: 13768.004 Loc: 0.855\n",
      "Step 1294: Loss: 13768.004 Loc: 0.855\n",
      "Step 1295: Loss: 13768.004 Loc: 0.855\n",
      "Step 1296: Loss: 13768.004 Loc: 0.855\n",
      "Step 1297: Loss: 13768.004 Loc: 0.855\n",
      "Step 1298: Loss: 13768.004 Loc: 0.855\n",
      "Step 1299: Loss: 13768.004 Loc: 0.855\n",
      "Step 1300: Loss: 13768.004 Loc: 0.855\n",
      "Step 1301: Loss: 13768.004 Loc: 0.855\n",
      "Step 1302: Loss: 13768.004 Loc: 0.855\n",
      "Step 1303: Loss: 13768.004 Loc: 0.855\n",
      "Step 1304: Loss: 13768.004 Loc: 0.855\n",
      "Step 1305: Loss: 13768.004 Loc: 0.855\n",
      "Step 1306: Loss: 13768.004 Loc: 0.855\n",
      "Step 1307: Loss: 13768.004 Loc: 0.855\n",
      "Step 1308: Loss: 13768.004 Loc: 0.855\n",
      "Step 1309: Loss: 13768.004 Loc: 0.855\n",
      "Step 1310: Loss: 13768.004 Loc: 0.855\n",
      "Step 1311: Loss: 13768.004 Loc: 0.855\n",
      "Step 1312: Loss: 13768.004 Loc: 0.855\n",
      "Step 1313: Loss: 13768.004 Loc: 0.855\n",
      "Step 1314: Loss: 13768.004 Loc: 0.855\n",
      "Step 1315: Loss: 13768.004 Loc: 0.855\n",
      "Step 1316: Loss: 13768.004 Loc: 0.855\n",
      "Step 1317: Loss: 13768.004 Loc: 0.855\n",
      "Step 1318: Loss: 13768.004 Loc: 0.855\n",
      "Step 1319: Loss: 13768.004 Loc: 0.855\n",
      "Step 1320: Loss: 13768.004 Loc: 0.855\n",
      "Step 1321: Loss: 13768.004 Loc: 0.855\n",
      "Step 1322: Loss: 13768.004 Loc: 0.855\n",
      "Step 1323: Loss: 13768.004 Loc: 0.855\n",
      "Step 1324: Loss: 13768.004 Loc: 0.855\n",
      "Step 1325: Loss: 13768.004 Loc: 0.855\n",
      "Step 1326: Loss: 13768.004 Loc: 0.855\n",
      "Step 1327: Loss: 13768.004 Loc: 0.855\n",
      "Step 1328: Loss: 13768.004 Loc: 0.855\n",
      "Step 1329: Loss: 13768.004 Loc: 0.855\n",
      "Step 1330: Loss: 13768.004 Loc: 0.855\n",
      "Step 1331: Loss: 13768.004 Loc: 0.855\n",
      "Step 1332: Loss: 13768.004 Loc: 0.855\n",
      "Step 1333: Loss: 13768.004 Loc: 0.855\n",
      "Step 1334: Loss: 13768.004 Loc: 0.855\n",
      "Step 1335: Loss: 13768.004 Loc: 0.855\n",
      "Step 1336: Loss: 13768.004 Loc: 0.855\n",
      "Step 1337: Loss: 13768.004 Loc: 0.855\n",
      "Step 1338: Loss: 13768.004 Loc: 0.855\n",
      "Step 1339: Loss: 13768.004 Loc: 0.855\n",
      "Step 1340: Loss: 13768.004 Loc: 0.855\n",
      "Step 1341: Loss: 13768.004 Loc: 0.855\n",
      "Step 1342: Loss: 13768.004 Loc: 0.855\n",
      "Step 1343: Loss: 13768.004 Loc: 0.855\n",
      "Step 1344: Loss: 13768.004 Loc: 0.855\n",
      "Step 1345: Loss: 13768.004 Loc: 0.855\n",
      "Step 1346: Loss: 13768.004 Loc: 0.855\n",
      "Step 1347: Loss: 13768.004 Loc: 0.855\n",
      "Step 1348: Loss: 13768.004 Loc: 0.855\n",
      "Step 1349: Loss: 13768.004 Loc: 0.855\n",
      "Step 1350: Loss: 13768.004 Loc: 0.855\n",
      "Step 1351: Loss: 13768.004 Loc: 0.855\n",
      "Step 1352: Loss: 13768.004 Loc: 0.855\n",
      "Step 1353: Loss: 13768.004 Loc: 0.855\n",
      "Step 1354: Loss: 13768.004 Loc: 0.855\n",
      "Step 1355: Loss: 13768.004 Loc: 0.855\n",
      "Step 1356: Loss: 13768.004 Loc: 0.855\n",
      "Step 1357: Loss: 13768.004 Loc: 0.855\n",
      "Step 1358: Loss: 13768.004 Loc: 0.855\n",
      "Step 1359: Loss: 13768.004 Loc: 0.855\n",
      "Step 1360: Loss: 13768.004 Loc: 0.855\n",
      "Step 1361: Loss: 13768.004 Loc: 0.855\n",
      "Step 1362: Loss: 13768.004 Loc: 0.855\n",
      "Step 1363: Loss: 13768.004 Loc: 0.855\n",
      "Step 1364: Loss: 13768.004 Loc: 0.855\n",
      "Step 1365: Loss: 13768.004 Loc: 0.855\n",
      "Step 1366: Loss: 13768.004 Loc: 0.855\n",
      "Step 1367: Loss: 13768.004 Loc: 0.855\n",
      "Step 1368: Loss: 13768.004 Loc: 0.855\n",
      "Step 1369: Loss: 13768.004 Loc: 0.855\n",
      "Step 1370: Loss: 13768.004 Loc: 0.855\n",
      "Step 1371: Loss: 13768.004 Loc: 0.855\n",
      "Step 1372: Loss: 13768.004 Loc: 0.855\n",
      "Step 1373: Loss: 13768.004 Loc: 0.855\n",
      "Step 1374: Loss: 13768.004 Loc: 0.855\n",
      "Step 1375: Loss: 13768.004 Loc: 0.855\n",
      "Step 1376: Loss: 13768.004 Loc: 0.855\n",
      "Step 1377: Loss: 13768.004 Loc: 0.855\n",
      "Step 1378: Loss: 13768.004 Loc: 0.855\n",
      "Step 1379: Loss: 13768.004 Loc: 0.855\n",
      "Step 1380: Loss: 13768.004 Loc: 0.855\n",
      "Step 1381: Loss: 13768.004 Loc: 0.855\n",
      "Step 1382: Loss: 13768.004 Loc: 0.855\n",
      "Step 1383: Loss: 13768.004 Loc: 0.855\n",
      "Step 1384: Loss: 13768.004 Loc: 0.855\n",
      "Step 1385: Loss: 13768.004 Loc: 0.855\n",
      "Step 1386: Loss: 13768.004 Loc: 0.855\n",
      "Step 1387: Loss: 13768.004 Loc: 0.855\n",
      "Step 1388: Loss: 13768.004 Loc: 0.855\n",
      "Step 1389: Loss: 13768.004 Loc: 0.855\n",
      "Step 1390: Loss: 13768.004 Loc: 0.855\n",
      "Step 1391: Loss: 13768.004 Loc: 0.855\n",
      "Step 1392: Loss: 13768.004 Loc: 0.855\n",
      "Step 1393: Loss: 13768.004 Loc: 0.855\n",
      "Step 1394: Loss: 13768.004 Loc: 0.855\n",
      "Step 1395: Loss: 13768.004 Loc: 0.855\n",
      "Step 1396: Loss: 13768.004 Loc: 0.855\n",
      "Step 1397: Loss: 13768.004 Loc: 0.855\n",
      "Step 1398: Loss: 13768.004 Loc: 0.855\n",
      "Step 1399: Loss: 13768.004 Loc: 0.855\n",
      "Step 1400: Loss: 13768.004 Loc: 0.855\n",
      "Step 1401: Loss: 13768.004 Loc: 0.855\n",
      "Step 1402: Loss: 13768.004 Loc: 0.855\n",
      "Step 1403: Loss: 13768.004 Loc: 0.855\n",
      "Step 1404: Loss: 13768.004 Loc: 0.855\n",
      "Step 1405: Loss: 13768.004 Loc: 0.855\n",
      "Step 1406: Loss: 13768.004 Loc: 0.855\n",
      "Step 1407: Loss: 13768.004 Loc: 0.855\n",
      "Step 1408: Loss: 13768.004 Loc: 0.855\n",
      "Step 1409: Loss: 13768.004 Loc: 0.855\n",
      "Step 1410: Loss: 13768.004 Loc: 0.855\n",
      "Step 1411: Loss: 13768.004 Loc: 0.855\n",
      "Step 1412: Loss: 13768.004 Loc: 0.855\n",
      "Step 1413: Loss: 13768.004 Loc: 0.855\n",
      "Step 1414: Loss: 13768.004 Loc: 0.855\n",
      "Step 1415: Loss: 13768.004 Loc: 0.855\n",
      "Step 1416: Loss: 13768.004 Loc: 0.855\n",
      "Step 1417: Loss: 13768.004 Loc: 0.855\n",
      "Step 1418: Loss: 13768.004 Loc: 0.855\n",
      "Step 1419: Loss: 13768.004 Loc: 0.855\n",
      "Step 1420: Loss: 13768.004 Loc: 0.855\n",
      "Step 1421: Loss: 13768.004 Loc: 0.855\n",
      "Step 1422: Loss: 13768.004 Loc: 0.855\n",
      "Step 1423: Loss: 13768.004 Loc: 0.855\n",
      "Step 1424: Loss: 13768.004 Loc: 0.855\n",
      "Step 1425: Loss: 13768.004 Loc: 0.855\n",
      "Step 1426: Loss: 13768.004 Loc: 0.855\n",
      "Step 1427: Loss: 13768.004 Loc: 0.855\n",
      "Step 1428: Loss: 13768.004 Loc: 0.855\n",
      "Step 1429: Loss: 13768.004 Loc: 0.855\n",
      "Step 1430: Loss: 13768.004 Loc: 0.855\n",
      "Step 1431: Loss: 13768.004 Loc: 0.855\n",
      "Step 1432: Loss: 13768.004 Loc: 0.855\n",
      "Step 1433: Loss: 13768.004 Loc: 0.855\n",
      "Step 1434: Loss: 13768.004 Loc: 0.855\n",
      "Step 1435: Loss: 13768.004 Loc: 0.855\n",
      "Step 1436: Loss: 13768.004 Loc: 0.855\n",
      "Step 1437: Loss: 13768.004 Loc: 0.855\n",
      "Step 1438: Loss: 13768.004 Loc: 0.855\n",
      "Step 1439: Loss: 13768.004 Loc: 0.855\n",
      "Step 1440: Loss: 13768.004 Loc: 0.855\n",
      "Step 1441: Loss: 13768.004 Loc: 0.855\n",
      "Step 1442: Loss: 13768.004 Loc: 0.855\n",
      "Step 1443: Loss: 13768.004 Loc: 0.855\n",
      "Step 1444: Loss: 13768.004 Loc: 0.855\n",
      "Step 1445: Loss: 13768.004 Loc: 0.855\n",
      "Step 1446: Loss: 13768.004 Loc: 0.855\n",
      "Step 1447: Loss: 13768.004 Loc: 0.855\n",
      "Step 1448: Loss: 13768.004 Loc: 0.855\n",
      "Step 1449: Loss: 13768.004 Loc: 0.855\n",
      "Step 1450: Loss: 13768.004 Loc: 0.855\n",
      "Step 1451: Loss: 13768.004 Loc: 0.855\n",
      "Step 1452: Loss: 13768.004 Loc: 0.855\n",
      "Step 1453: Loss: 13768.004 Loc: 0.855\n",
      "Step 1454: Loss: 13768.004 Loc: 0.855\n",
      "Step 1455: Loss: 13768.004 Loc: 0.855\n",
      "Step 1456: Loss: 13768.004 Loc: 0.855\n",
      "Step 1457: Loss: 13768.004 Loc: 0.855\n",
      "Step 1458: Loss: 13768.004 Loc: 0.855\n",
      "Step 1459: Loss: 13768.004 Loc: 0.855\n",
      "Step 1460: Loss: 13768.004 Loc: 0.855\n",
      "Step 1461: Loss: 13768.004 Loc: 0.855\n",
      "Step 1462: Loss: 13768.004 Loc: 0.855\n",
      "Step 1463: Loss: 13768.004 Loc: 0.855\n",
      "Step 1464: Loss: 13768.004 Loc: 0.855\n",
      "Step 1465: Loss: 13768.004 Loc: 0.855\n",
      "Step 1466: Loss: 13768.004 Loc: 0.855\n",
      "Step 1467: Loss: 13768.004 Loc: 0.855\n",
      "Step 1468: Loss: 13768.004 Loc: 0.855\n",
      "Step 1469: Loss: 13768.004 Loc: 0.855\n",
      "Step 1470: Loss: 13768.004 Loc: 0.855\n",
      "Step 1471: Loss: 13768.004 Loc: 0.855\n",
      "Step 1472: Loss: 13768.004 Loc: 0.855\n",
      "Step 1473: Loss: 13768.004 Loc: 0.855\n",
      "Step 1474: Loss: 13768.004 Loc: 0.855\n",
      "Step 1475: Loss: 13768.004 Loc: 0.855\n",
      "Step 1476: Loss: 13768.004 Loc: 0.855\n",
      "Step 1477: Loss: 13768.004 Loc: 0.855\n",
      "Step 1478: Loss: 13768.004 Loc: 0.855\n",
      "Step 1479: Loss: 13768.004 Loc: 0.855\n",
      "Step 1480: Loss: 13768.004 Loc: 0.855\n",
      "Step 1481: Loss: 13768.004 Loc: 0.855\n",
      "Step 1482: Loss: 13768.004 Loc: 0.855\n",
      "Step 1483: Loss: 13768.004 Loc: 0.855\n",
      "Step 1484: Loss: 13768.004 Loc: 0.855\n",
      "Step 1485: Loss: 13768.004 Loc: 0.855\n",
      "Step 1486: Loss: 13768.004 Loc: 0.855\n",
      "Step 1487: Loss: 13768.004 Loc: 0.855\n",
      "Step 1488: Loss: 13768.004 Loc: 0.855\n",
      "Step 1489: Loss: 13768.004 Loc: 0.855\n",
      "Step 1490: Loss: 13768.004 Loc: 0.855\n",
      "Step 1491: Loss: 13768.004 Loc: 0.855\n",
      "Step 1492: Loss: 13768.004 Loc: 0.855\n",
      "Step 1493: Loss: 13768.004 Loc: 0.855\n",
      "Step 1494: Loss: 13768.004 Loc: 0.855\n",
      "Step 1495: Loss: 13768.004 Loc: 0.855\n",
      "Step 1496: Loss: 13768.004 Loc: 0.855\n",
      "Step 1497: Loss: 13768.004 Loc: 0.855\n",
      "Step 1498: Loss: 13768.004 Loc: 0.855\n",
      "Step 1499: Loss: 13768.004 Loc: 0.855\n",
      "Step 1500: Loss: 13768.004 Loc: 0.855\n",
      "Step 1501: Loss: 13768.004 Loc: 0.855\n",
      "Step 1502: Loss: 13768.004 Loc: 0.855\n",
      "Step 1503: Loss: 13768.004 Loc: 0.855\n",
      "Step 1504: Loss: 13768.004 Loc: 0.855\n",
      "Step 1505: Loss: 13768.004 Loc: 0.855\n",
      "Step 1506: Loss: 13768.004 Loc: 0.855\n",
      "Step 1507: Loss: 13768.004 Loc: 0.855\n",
      "Step 1508: Loss: 13768.004 Loc: 0.855\n",
      "Step 1509: Loss: 13768.004 Loc: 0.855\n",
      "Step 1510: Loss: 13768.004 Loc: 0.855\n",
      "Step 1511: Loss: 13768.004 Loc: 0.855\n",
      "Step 1512: Loss: 13768.004 Loc: 0.855\n",
      "Step 1513: Loss: 13768.004 Loc: 0.855\n",
      "Step 1514: Loss: 13768.004 Loc: 0.855\n",
      "Step 1515: Loss: 13768.004 Loc: 0.855\n",
      "Step 1516: Loss: 13768.004 Loc: 0.855\n",
      "Step 1517: Loss: 13768.004 Loc: 0.855\n",
      "Step 1518: Loss: 13768.004 Loc: 0.855\n",
      "Step 1519: Loss: 13768.004 Loc: 0.855\n",
      "Step 1520: Loss: 13768.004 Loc: 0.855\n",
      "Step 1521: Loss: 13768.004 Loc: 0.855\n",
      "Step 1522: Loss: 13768.004 Loc: 0.855\n",
      "Step 1523: Loss: 13768.004 Loc: 0.855\n",
      "Step 1524: Loss: 13768.004 Loc: 0.855\n",
      "Step 1525: Loss: 13768.004 Loc: 0.855\n",
      "Step 1526: Loss: 13768.004 Loc: 0.855\n",
      "Step 1527: Loss: 13768.004 Loc: 0.855\n",
      "Step 1528: Loss: 13768.004 Loc: 0.855\n",
      "Step 1529: Loss: 13768.004 Loc: 0.855\n",
      "Step 1530: Loss: 13768.004 Loc: 0.855\n",
      "Step 1531: Loss: 13768.004 Loc: 0.855\n",
      "Step 1532: Loss: 13768.004 Loc: 0.855\n",
      "Step 1533: Loss: 13768.004 Loc: 0.855\n",
      "Step 1534: Loss: 13768.004 Loc: 0.855\n",
      "Step 1535: Loss: 13768.004 Loc: 0.855\n",
      "Step 1536: Loss: 13768.004 Loc: 0.855\n",
      "Step 1537: Loss: 13768.004 Loc: 0.855\n",
      "Step 1538: Loss: 13768.004 Loc: 0.855\n",
      "Step 1539: Loss: 13768.004 Loc: 0.855\n",
      "Step 1540: Loss: 13768.004 Loc: 0.855\n",
      "Step 1541: Loss: 13768.004 Loc: 0.855\n",
      "Step 1542: Loss: 13768.004 Loc: 0.855\n",
      "Step 1543: Loss: 13768.004 Loc: 0.855\n",
      "Step 1544: Loss: 13768.004 Loc: 0.855\n",
      "Step 1545: Loss: 13768.004 Loc: 0.855\n",
      "Step 1546: Loss: 13768.004 Loc: 0.855\n",
      "Step 1547: Loss: 13768.004 Loc: 0.855\n",
      "Step 1548: Loss: 13768.004 Loc: 0.855\n",
      "Step 1549: Loss: 13768.004 Loc: 0.855\n",
      "Step 1550: Loss: 13768.004 Loc: 0.855\n",
      "Step 1551: Loss: 13768.004 Loc: 0.855\n",
      "Step 1552: Loss: 13768.004 Loc: 0.855\n",
      "Step 1553: Loss: 13768.004 Loc: 0.855\n",
      "Step 1554: Loss: 13768.004 Loc: 0.855\n",
      "Step 1555: Loss: 13768.004 Loc: 0.855\n",
      "Step 1556: Loss: 13768.004 Loc: 0.855\n",
      "Step 1557: Loss: 13768.004 Loc: 0.855\n",
      "Step 1558: Loss: 13768.004 Loc: 0.855\n",
      "Step 1559: Loss: 13768.004 Loc: 0.855\n",
      "Step 1560: Loss: 13768.004 Loc: 0.855\n",
      "Step 1561: Loss: 13768.004 Loc: 0.855\n",
      "Step 1562: Loss: 13768.004 Loc: 0.855\n",
      "Step 1563: Loss: 13768.004 Loc: 0.855\n",
      "Step 1564: Loss: 13768.004 Loc: 0.855\n",
      "Step 1565: Loss: 13768.004 Loc: 0.855\n",
      "Step 1566: Loss: 13768.004 Loc: 0.855\n",
      "Step 1567: Loss: 13768.004 Loc: 0.855\n",
      "Step 1568: Loss: 13768.004 Loc: 0.855\n",
      "Step 1569: Loss: 13768.004 Loc: 0.855\n",
      "Step 1570: Loss: 13768.004 Loc: 0.855\n",
      "Step 1571: Loss: 13768.004 Loc: 0.855\n",
      "Step 1572: Loss: 13768.004 Loc: 0.855\n",
      "Step 1573: Loss: 13768.004 Loc: 0.855\n",
      "Step 1574: Loss: 13768.004 Loc: 0.855\n",
      "Step 1575: Loss: 13768.004 Loc: 0.855\n",
      "Step 1576: Loss: 13768.004 Loc: 0.855\n",
      "Step 1577: Loss: 13768.004 Loc: 0.855\n",
      "Step 1578: Loss: 13768.004 Loc: 0.855\n",
      "Step 1579: Loss: 13768.004 Loc: 0.855\n",
      "Step 1580: Loss: 13768.004 Loc: 0.855\n",
      "Step 1581: Loss: 13768.004 Loc: 0.855\n",
      "Step 1582: Loss: 13768.004 Loc: 0.855\n",
      "Step 1583: Loss: 13768.004 Loc: 0.855\n",
      "Step 1584: Loss: 13768.004 Loc: 0.855\n",
      "Step 1585: Loss: 13768.004 Loc: 0.855\n",
      "Step 1586: Loss: 13768.004 Loc: 0.855\n",
      "Step 1587: Loss: 13768.004 Loc: 0.855\n",
      "Step 1588: Loss: 13768.004 Loc: 0.855\n",
      "Step 1589: Loss: 13768.004 Loc: 0.855\n",
      "Step 1590: Loss: 13768.004 Loc: 0.855\n",
      "Step 1591: Loss: 13768.004 Loc: 0.855\n",
      "Step 1592: Loss: 13768.004 Loc: 0.855\n",
      "Step 1593: Loss: 13768.004 Loc: 0.855\n",
      "Step 1594: Loss: 13768.004 Loc: 0.855\n",
      "Step 1595: Loss: 13768.004 Loc: 0.855\n",
      "Step 1596: Loss: 13768.004 Loc: 0.855\n",
      "Step 1597: Loss: 13768.004 Loc: 0.855\n",
      "Step 1598: Loss: 13768.004 Loc: 0.855\n",
      "Step 1599: Loss: 13768.004 Loc: 0.855\n",
      "Step 1600: Loss: 13768.004 Loc: 0.855\n",
      "Step 1601: Loss: 13768.004 Loc: 0.855\n",
      "Step 1602: Loss: 13768.004 Loc: 0.855\n",
      "Step 1603: Loss: 13768.004 Loc: 0.855\n",
      "Step 1604: Loss: 13768.004 Loc: 0.855\n",
      "Step 1605: Loss: 13768.004 Loc: 0.855\n",
      "Step 1606: Loss: 13768.004 Loc: 0.855\n",
      "Step 1607: Loss: 13768.004 Loc: 0.855\n",
      "Step 1608: Loss: 13768.004 Loc: 0.855\n",
      "Step 1609: Loss: 13768.004 Loc: 0.855\n",
      "Step 1610: Loss: 13768.004 Loc: 0.855\n",
      "Step 1611: Loss: 13768.004 Loc: 0.855\n",
      "Step 1612: Loss: 13768.004 Loc: 0.855\n",
      "Step 1613: Loss: 13768.004 Loc: 0.855\n",
      "Step 1614: Loss: 13768.004 Loc: 0.855\n",
      "Step 1615: Loss: 13768.004 Loc: 0.855\n",
      "Step 1616: Loss: 13768.004 Loc: 0.855\n",
      "Step 1617: Loss: 13768.004 Loc: 0.855\n",
      "Step 1618: Loss: 13768.004 Loc: 0.855\n",
      "Step 1619: Loss: 13768.004 Loc: 0.855\n",
      "Step 1620: Loss: 13768.004 Loc: 0.855\n",
      "Step 1621: Loss: 13768.004 Loc: 0.855\n",
      "Step 1622: Loss: 13768.004 Loc: 0.855\n",
      "Step 1623: Loss: 13768.004 Loc: 0.855\n",
      "Step 1624: Loss: 13768.004 Loc: 0.855\n",
      "Step 1625: Loss: 13768.004 Loc: 0.855\n",
      "Step 1626: Loss: 13768.004 Loc: 0.855\n",
      "Step 1627: Loss: 13768.004 Loc: 0.855\n",
      "Step 1628: Loss: 13768.004 Loc: 0.855\n",
      "Step 1629: Loss: 13768.004 Loc: 0.855\n",
      "Step 1630: Loss: 13768.004 Loc: 0.855\n",
      "Step 1631: Loss: 13768.004 Loc: 0.855\n",
      "Step 1632: Loss: 13768.004 Loc: 0.855\n",
      "Step 1633: Loss: 13768.004 Loc: 0.855\n",
      "Step 1634: Loss: 13768.004 Loc: 0.855\n",
      "Step 1635: Loss: 13768.004 Loc: 0.855\n",
      "Step 1636: Loss: 13768.004 Loc: 0.855\n",
      "Step 1637: Loss: 13768.004 Loc: 0.855\n",
      "Step 1638: Loss: 13768.004 Loc: 0.855\n",
      "Step 1639: Loss: 13768.004 Loc: 0.855\n",
      "Step 1640: Loss: 13768.004 Loc: 0.855\n",
      "Step 1641: Loss: 13768.004 Loc: 0.855\n",
      "Step 1642: Loss: 13768.004 Loc: 0.855\n",
      "Step 1643: Loss: 13768.004 Loc: 0.855\n",
      "Step 1644: Loss: 13768.004 Loc: 0.855\n",
      "Step 1645: Loss: 13768.004 Loc: 0.855\n",
      "Step 1646: Loss: 13768.004 Loc: 0.855\n",
      "Step 1647: Loss: 13768.004 Loc: 0.855\n",
      "Step 1648: Loss: 13768.004 Loc: 0.855\n",
      "Step 1649: Loss: 13768.004 Loc: 0.855\n",
      "Step 1650: Loss: 13768.004 Loc: 0.855\n",
      "Step 1651: Loss: 13768.004 Loc: 0.855\n",
      "Step 1652: Loss: 13768.004 Loc: 0.855\n",
      "Step 1653: Loss: 13768.004 Loc: 0.855\n",
      "Step 1654: Loss: 13768.004 Loc: 0.855\n",
      "Step 1655: Loss: 13768.004 Loc: 0.855\n",
      "Step 1656: Loss: 13768.004 Loc: 0.855\n",
      "Step 1657: Loss: 13768.004 Loc: 0.855\n",
      "Step 1658: Loss: 13768.004 Loc: 0.855\n",
      "Step 1659: Loss: 13768.004 Loc: 0.855\n",
      "Step 1660: Loss: 13768.004 Loc: 0.855\n",
      "Step 1661: Loss: 13768.004 Loc: 0.855\n",
      "Step 1662: Loss: 13768.004 Loc: 0.855\n",
      "Step 1663: Loss: 13768.004 Loc: 0.855\n",
      "Step 1664: Loss: 13768.004 Loc: 0.855\n",
      "Step 1665: Loss: 13768.004 Loc: 0.855\n",
      "Step 1666: Loss: 13768.004 Loc: 0.855\n",
      "Step 1667: Loss: 13768.004 Loc: 0.855\n",
      "Step 1668: Loss: 13768.004 Loc: 0.855\n",
      "Step 1669: Loss: 13768.004 Loc: 0.855\n",
      "Step 1670: Loss: 13768.004 Loc: 0.855\n",
      "Step 1671: Loss: 13768.004 Loc: 0.855\n",
      "Step 1672: Loss: 13768.004 Loc: 0.855\n",
      "Step 1673: Loss: 13768.004 Loc: 0.855\n",
      "Step 1674: Loss: 13768.004 Loc: 0.855\n",
      "Step 1675: Loss: 13768.004 Loc: 0.855\n",
      "Step 1676: Loss: 13768.004 Loc: 0.855\n",
      "Step 1677: Loss: 13768.004 Loc: 0.855\n",
      "Step 1678: Loss: 13768.004 Loc: 0.855\n",
      "Step 1679: Loss: 13768.004 Loc: 0.855\n",
      "Step 1680: Loss: 13768.004 Loc: 0.855\n",
      "Step 1681: Loss: 13768.004 Loc: 0.855\n",
      "Step 1682: Loss: 13768.004 Loc: 0.855\n",
      "Step 1683: Loss: 13768.004 Loc: 0.855\n",
      "Step 1684: Loss: 13768.004 Loc: 0.855\n",
      "Step 1685: Loss: 13768.004 Loc: 0.855\n",
      "Step 1686: Loss: 13768.004 Loc: 0.855\n",
      "Step 1687: Loss: 13768.004 Loc: 0.855\n",
      "Step 1688: Loss: 13768.004 Loc: 0.855\n",
      "Step 1689: Loss: 13768.004 Loc: 0.855\n",
      "Step 1690: Loss: 13768.004 Loc: 0.855\n",
      "Step 1691: Loss: 13768.004 Loc: 0.855\n",
      "Step 1692: Loss: 13768.004 Loc: 0.855\n",
      "Step 1693: Loss: 13768.004 Loc: 0.855\n",
      "Step 1694: Loss: 13768.004 Loc: 0.855\n",
      "Step 1695: Loss: 13768.004 Loc: 0.855\n",
      "Step 1696: Loss: 13768.004 Loc: 0.855\n",
      "Step 1697: Loss: 13768.004 Loc: 0.855\n",
      "Step 1698: Loss: 13768.004 Loc: 0.855\n",
      "Step 1699: Loss: 13768.004 Loc: 0.855\n",
      "Step 1700: Loss: 13768.004 Loc: 0.855\n",
      "Step 1701: Loss: 13768.004 Loc: 0.855\n",
      "Step 1702: Loss: 13768.004 Loc: 0.855\n",
      "Step 1703: Loss: 13768.004 Loc: 0.855\n",
      "Step 1704: Loss: 13768.004 Loc: 0.855\n",
      "Step 1705: Loss: 13768.004 Loc: 0.855\n",
      "Step 1706: Loss: 13768.004 Loc: 0.855\n",
      "Step 1707: Loss: 13768.004 Loc: 0.855\n",
      "Step 1708: Loss: 13768.004 Loc: 0.855\n",
      "Step 1709: Loss: 13768.004 Loc: 0.855\n",
      "Step 1710: Loss: 13768.004 Loc: 0.855\n",
      "Step 1711: Loss: 13768.004 Loc: 0.855\n",
      "Step 1712: Loss: 13768.004 Loc: 0.855\n",
      "Step 1713: Loss: 13768.004 Loc: 0.855\n",
      "Step 1714: Loss: 13768.004 Loc: 0.855\n",
      "Step 1715: Loss: 13768.004 Loc: 0.855\n",
      "Step 1716: Loss: 13768.004 Loc: 0.855\n",
      "Step 1717: Loss: 13768.004 Loc: 0.855\n",
      "Step 1718: Loss: 13768.004 Loc: 0.855\n",
      "Step 1719: Loss: 13768.004 Loc: 0.855\n",
      "Step 1720: Loss: 13768.004 Loc: 0.855\n",
      "Step 1721: Loss: 13768.004 Loc: 0.855\n",
      "Step 1722: Loss: 13768.004 Loc: 0.855\n",
      "Step 1723: Loss: 13768.004 Loc: 0.855\n",
      "Step 1724: Loss: 13768.004 Loc: 0.855\n",
      "Step 1725: Loss: 13768.004 Loc: 0.855\n",
      "Step 1726: Loss: 13768.004 Loc: 0.855\n",
      "Step 1727: Loss: 13768.004 Loc: 0.855\n",
      "Step 1728: Loss: 13768.004 Loc: 0.855\n",
      "Step 1729: Loss: 13768.004 Loc: 0.855\n",
      "Step 1730: Loss: 13768.004 Loc: 0.855\n",
      "Step 1731: Loss: 13768.004 Loc: 0.855\n",
      "Step 1732: Loss: 13768.004 Loc: 0.855\n",
      "Step 1733: Loss: 13768.004 Loc: 0.855\n",
      "Step 1734: Loss: 13768.004 Loc: 0.855\n",
      "Step 1735: Loss: 13768.004 Loc: 0.855\n",
      "Step 1736: Loss: 13768.004 Loc: 0.855\n",
      "Step 1737: Loss: 13768.004 Loc: 0.855\n",
      "Step 1738: Loss: 13768.004 Loc: 0.855\n",
      "Step 1739: Loss: 13768.004 Loc: 0.855\n",
      "Step 1740: Loss: 13768.004 Loc: 0.855\n",
      "Step 1741: Loss: 13768.004 Loc: 0.855\n",
      "Step 1742: Loss: 13768.004 Loc: 0.855\n",
      "Step 1743: Loss: 13768.004 Loc: 0.855\n",
      "Step 1744: Loss: 13768.004 Loc: 0.855\n",
      "Step 1745: Loss: 13768.004 Loc: 0.855\n",
      "Step 1746: Loss: 13768.004 Loc: 0.855\n",
      "Step 1747: Loss: 13768.004 Loc: 0.855\n",
      "Step 1748: Loss: 13768.004 Loc: 0.855\n",
      "Step 1749: Loss: 13768.004 Loc: 0.855\n",
      "Step 1750: Loss: 13768.004 Loc: 0.855\n",
      "Step 1751: Loss: 13768.004 Loc: 0.855\n",
      "Step 1752: Loss: 13768.004 Loc: 0.855\n",
      "Step 1753: Loss: 13768.004 Loc: 0.855\n",
      "Step 1754: Loss: 13768.004 Loc: 0.855\n",
      "Step 1755: Loss: 13768.004 Loc: 0.855\n",
      "Step 1756: Loss: 13768.004 Loc: 0.855\n",
      "Step 1757: Loss: 13768.004 Loc: 0.855\n",
      "Step 1758: Loss: 13768.004 Loc: 0.855\n",
      "Step 1759: Loss: 13768.004 Loc: 0.855\n",
      "Step 1760: Loss: 13768.004 Loc: 0.855\n",
      "Step 1761: Loss: 13768.004 Loc: 0.855\n",
      "Step 1762: Loss: 13768.004 Loc: 0.855\n",
      "Step 1763: Loss: 13768.004 Loc: 0.855\n",
      "Step 1764: Loss: 13768.004 Loc: 0.855\n",
      "Step 1765: Loss: 13768.004 Loc: 0.855\n",
      "Step 1766: Loss: 13768.004 Loc: 0.855\n",
      "Step 1767: Loss: 13768.004 Loc: 0.855\n",
      "Step 1768: Loss: 13768.004 Loc: 0.855\n",
      "Step 1769: Loss: 13768.004 Loc: 0.855\n",
      "Step 1770: Loss: 13768.004 Loc: 0.855\n",
      "Step 1771: Loss: 13768.004 Loc: 0.855\n",
      "Step 1772: Loss: 13768.004 Loc: 0.855\n",
      "Step 1773: Loss: 13768.004 Loc: 0.855\n",
      "Step 1774: Loss: 13768.004 Loc: 0.855\n",
      "Step 1775: Loss: 13768.004 Loc: 0.855\n",
      "Step 1776: Loss: 13768.004 Loc: 0.855\n",
      "Step 1777: Loss: 13768.004 Loc: 0.855\n",
      "Step 1778: Loss: 13768.004 Loc: 0.855\n",
      "Step 1779: Loss: 13768.004 Loc: 0.855\n",
      "Step 1780: Loss: 13768.004 Loc: 0.855\n",
      "Step 1781: Loss: 13768.004 Loc: 0.855\n",
      "Step 1782: Loss: 13768.004 Loc: 0.855\n",
      "Step 1783: Loss: 13768.004 Loc: 0.855\n",
      "Step 1784: Loss: 13768.004 Loc: 0.855\n",
      "Step 1785: Loss: 13768.004 Loc: 0.855\n",
      "Step 1786: Loss: 13768.004 Loc: 0.855\n",
      "Step 1787: Loss: 13768.004 Loc: 0.855\n",
      "Step 1788: Loss: 13768.004 Loc: 0.855\n",
      "Step 1789: Loss: 13768.004 Loc: 0.855\n",
      "Step 1790: Loss: 13768.004 Loc: 0.855\n",
      "Step 1791: Loss: 13768.004 Loc: 0.855\n",
      "Step 1792: Loss: 13768.004 Loc: 0.855\n",
      "Step 1793: Loss: 13768.004 Loc: 0.855\n",
      "Step 1794: Loss: 13768.004 Loc: 0.855\n",
      "Step 1795: Loss: 13768.004 Loc: 0.855\n",
      "Step 1796: Loss: 13768.004 Loc: 0.855\n",
      "Step 1797: Loss: 13768.004 Loc: 0.855\n",
      "Step 1798: Loss: 13768.004 Loc: 0.855\n",
      "Step 1799: Loss: 13768.004 Loc: 0.855\n",
      "Step 1800: Loss: 13768.004 Loc: 0.855\n",
      "Step 1801: Loss: 13768.004 Loc: 0.855\n",
      "Step 1802: Loss: 13768.004 Loc: 0.855\n",
      "Step 1803: Loss: 13768.004 Loc: 0.855\n",
      "Step 1804: Loss: 13768.004 Loc: 0.855\n",
      "Step 1805: Loss: 13768.004 Loc: 0.855\n",
      "Step 1806: Loss: 13768.004 Loc: 0.855\n",
      "Step 1807: Loss: 13768.004 Loc: 0.855\n",
      "Step 1808: Loss: 13768.004 Loc: 0.855\n",
      "Step 1809: Loss: 13768.004 Loc: 0.855\n",
      "Step 1810: Loss: 13768.004 Loc: 0.855\n",
      "Step 1811: Loss: 13768.004 Loc: 0.855\n",
      "Step 1812: Loss: 13768.004 Loc: 0.855\n",
      "Step 1813: Loss: 13768.004 Loc: 0.855\n",
      "Step 1814: Loss: 13768.004 Loc: 0.855\n",
      "Step 1815: Loss: 13768.004 Loc: 0.855\n",
      "Step 1816: Loss: 13768.004 Loc: 0.855\n",
      "Step 1817: Loss: 13768.004 Loc: 0.855\n",
      "Step 1818: Loss: 13768.004 Loc: 0.855\n",
      "Step 1819: Loss: 13768.004 Loc: 0.855\n",
      "Step 1820: Loss: 13768.004 Loc: 0.855\n",
      "Step 1821: Loss: 13768.004 Loc: 0.855\n",
      "Step 1822: Loss: 13768.004 Loc: 0.855\n",
      "Step 1823: Loss: 13768.004 Loc: 0.855\n",
      "Step 1824: Loss: 13768.004 Loc: 0.855\n",
      "Step 1825: Loss: 13768.004 Loc: 0.855\n",
      "Step 1826: Loss: 13768.004 Loc: 0.855\n",
      "Step 1827: Loss: 13768.004 Loc: 0.855\n",
      "Step 1828: Loss: 13768.004 Loc: 0.855\n",
      "Step 1829: Loss: 13768.004 Loc: 0.855\n",
      "Step 1830: Loss: 13768.004 Loc: 0.855\n",
      "Step 1831: Loss: 13768.004 Loc: 0.855\n",
      "Step 1832: Loss: 13768.004 Loc: 0.855\n",
      "Step 1833: Loss: 13768.004 Loc: 0.855\n",
      "Step 1834: Loss: 13768.004 Loc: 0.855\n",
      "Step 1835: Loss: 13768.004 Loc: 0.855\n",
      "Step 1836: Loss: 13768.004 Loc: 0.855\n",
      "Step 1837: Loss: 13768.004 Loc: 0.855\n",
      "Step 1838: Loss: 13768.004 Loc: 0.855\n",
      "Step 1839: Loss: 13768.004 Loc: 0.855\n",
      "Step 1840: Loss: 13768.004 Loc: 0.855\n",
      "Step 1841: Loss: 13768.004 Loc: 0.855\n",
      "Step 1842: Loss: 13768.004 Loc: 0.855\n",
      "Step 1843: Loss: 13768.004 Loc: 0.855\n",
      "Step 1844: Loss: 13768.004 Loc: 0.855\n",
      "Step 1845: Loss: 13768.004 Loc: 0.855\n",
      "Step 1846: Loss: 13768.004 Loc: 0.855\n",
      "Step 1847: Loss: 13768.004 Loc: 0.855\n",
      "Step 1848: Loss: 13768.004 Loc: 0.855\n",
      "Step 1849: Loss: 13768.004 Loc: 0.855\n",
      "Step 1850: Loss: 13768.004 Loc: 0.855\n",
      "Step 1851: Loss: 13768.004 Loc: 0.855\n",
      "Step 1852: Loss: 13768.004 Loc: 0.855\n",
      "Step 1853: Loss: 13768.004 Loc: 0.855\n",
      "Step 1854: Loss: 13768.004 Loc: 0.855\n",
      "Step 1855: Loss: 13768.004 Loc: 0.855\n",
      "Step 1856: Loss: 13768.004 Loc: 0.855\n",
      "Step 1857: Loss: 13768.004 Loc: 0.855\n",
      "Step 1858: Loss: 13768.004 Loc: 0.855\n",
      "Step 1859: Loss: 13768.004 Loc: 0.855\n",
      "Step 1860: Loss: 13768.004 Loc: 0.855\n",
      "Step 1861: Loss: 13768.004 Loc: 0.855\n",
      "Step 1862: Loss: 13768.004 Loc: 0.855\n",
      "Step 1863: Loss: 13768.004 Loc: 0.855\n",
      "Step 1864: Loss: 13768.004 Loc: 0.855\n",
      "Step 1865: Loss: 13768.004 Loc: 0.855\n",
      "Step 1866: Loss: 13768.004 Loc: 0.855\n",
      "Step 1867: Loss: 13768.004 Loc: 0.855\n",
      "Step 1868: Loss: 13768.004 Loc: 0.855\n",
      "Step 1869: Loss: 13768.004 Loc: 0.855\n",
      "Step 1870: Loss: 13768.004 Loc: 0.855\n",
      "Step 1871: Loss: 13768.004 Loc: 0.855\n",
      "Step 1872: Loss: 13768.004 Loc: 0.855\n",
      "Step 1873: Loss: 13768.004 Loc: 0.855\n",
      "Step 1874: Loss: 13768.004 Loc: 0.855\n",
      "Step 1875: Loss: 13768.004 Loc: 0.855\n",
      "Step 1876: Loss: 13768.004 Loc: 0.855\n",
      "Step 1877: Loss: 13768.004 Loc: 0.855\n",
      "Step 1878: Loss: 13768.004 Loc: 0.855\n",
      "Step 1879: Loss: 13768.004 Loc: 0.855\n",
      "Step 1880: Loss: 13768.004 Loc: 0.855\n",
      "Step 1881: Loss: 13768.004 Loc: 0.855\n",
      "Step 1882: Loss: 13768.004 Loc: 0.855\n",
      "Step 1883: Loss: 13768.004 Loc: 0.855\n",
      "Step 1884: Loss: 13768.004 Loc: 0.855\n",
      "Step 1885: Loss: 13768.004 Loc: 0.855\n",
      "Step 1886: Loss: 13768.004 Loc: 0.855\n",
      "Step 1887: Loss: 13768.004 Loc: 0.855\n",
      "Step 1888: Loss: 13768.004 Loc: 0.855\n",
      "Step 1889: Loss: 13768.004 Loc: 0.855\n",
      "Step 1890: Loss: 13768.004 Loc: 0.855\n",
      "Step 1891: Loss: 13768.004 Loc: 0.855\n",
      "Step 1892: Loss: 13768.004 Loc: 0.855\n",
      "Step 1893: Loss: 13768.004 Loc: 0.855\n",
      "Step 1894: Loss: 13768.004 Loc: 0.855\n",
      "Step 1895: Loss: 13768.004 Loc: 0.855\n",
      "Step 1896: Loss: 13768.004 Loc: 0.855\n",
      "Step 1897: Loss: 13768.004 Loc: 0.855\n",
      "Step 1898: Loss: 13768.004 Loc: 0.855\n",
      "Step 1899: Loss: 13768.004 Loc: 0.855\n",
      "Step 1900: Loss: 13768.004 Loc: 0.855\n",
      "Step 1901: Loss: 13768.004 Loc: 0.855\n",
      "Step 1902: Loss: 13768.004 Loc: 0.855\n",
      "Step 1903: Loss: 13768.004 Loc: 0.855\n",
      "Step 1904: Loss: 13768.004 Loc: 0.855\n",
      "Step 1905: Loss: 13768.004 Loc: 0.855\n",
      "Step 1906: Loss: 13768.004 Loc: 0.855\n",
      "Step 1907: Loss: 13768.004 Loc: 0.855\n",
      "Step 1908: Loss: 13768.004 Loc: 0.855\n",
      "Step 1909: Loss: 13768.004 Loc: 0.855\n",
      "Step 1910: Loss: 13768.004 Loc: 0.855\n",
      "Step 1911: Loss: 13768.004 Loc: 0.855\n",
      "Step 1912: Loss: 13768.004 Loc: 0.855\n",
      "Step 1913: Loss: 13768.004 Loc: 0.855\n",
      "Step 1914: Loss: 13768.004 Loc: 0.855\n",
      "Step 1915: Loss: 13768.004 Loc: 0.855\n",
      "Step 1916: Loss: 13768.004 Loc: 0.855\n",
      "Step 1917: Loss: 13768.004 Loc: 0.855\n",
      "Step 1918: Loss: 13768.004 Loc: 0.855\n",
      "Step 1919: Loss: 13768.004 Loc: 0.855\n",
      "Step 1920: Loss: 13768.004 Loc: 0.855\n",
      "Step 1921: Loss: 13768.004 Loc: 0.855\n",
      "Step 1922: Loss: 13768.004 Loc: 0.855\n",
      "Step 1923: Loss: 13768.004 Loc: 0.855\n",
      "Step 1924: Loss: 13768.004 Loc: 0.855\n",
      "Step 1925: Loss: 13768.004 Loc: 0.855\n",
      "Step 1926: Loss: 13768.004 Loc: 0.855\n",
      "Step 1927: Loss: 13768.004 Loc: 0.855\n",
      "Step 1928: Loss: 13768.004 Loc: 0.855\n",
      "Step 1929: Loss: 13768.004 Loc: 0.855\n",
      "Step 1930: Loss: 13768.004 Loc: 0.855\n",
      "Step 1931: Loss: 13768.004 Loc: 0.855\n",
      "Step 1932: Loss: 13768.004 Loc: 0.855\n",
      "Step 1933: Loss: 13768.004 Loc: 0.855\n",
      "Step 1934: Loss: 13768.004 Loc: 0.855\n",
      "Step 1935: Loss: 13768.004 Loc: 0.855\n",
      "Step 1936: Loss: 13768.004 Loc: 0.855\n",
      "Step 1937: Loss: 13768.004 Loc: 0.855\n",
      "Step 1938: Loss: 13768.004 Loc: 0.855\n",
      "Step 1939: Loss: 13768.004 Loc: 0.855\n",
      "Step 1940: Loss: 13768.004 Loc: 0.855\n",
      "Step 1941: Loss: 13768.004 Loc: 0.855\n",
      "Step 1942: Loss: 13768.004 Loc: 0.855\n",
      "Step 1943: Loss: 13768.004 Loc: 0.855\n",
      "Step 1944: Loss: 13768.004 Loc: 0.855\n",
      "Step 1945: Loss: 13768.004 Loc: 0.855\n",
      "Step 1946: Loss: 13768.004 Loc: 0.855\n",
      "Step 1947: Loss: 13768.004 Loc: 0.855\n",
      "Step 1948: Loss: 13768.004 Loc: 0.855\n",
      "Step 1949: Loss: 13768.004 Loc: 0.855\n",
      "Step 1950: Loss: 13768.004 Loc: 0.855\n",
      "Step 1951: Loss: 13768.004 Loc: 0.855\n",
      "Step 1952: Loss: 13768.004 Loc: 0.855\n",
      "Step 1953: Loss: 13768.004 Loc: 0.855\n",
      "Step 1954: Loss: 13768.004 Loc: 0.855\n",
      "Step 1955: Loss: 13768.004 Loc: 0.855\n",
      "Step 1956: Loss: 13768.004 Loc: 0.855\n",
      "Step 1957: Loss: 13768.004 Loc: 0.855\n",
      "Step 1958: Loss: 13768.004 Loc: 0.855\n",
      "Step 1959: Loss: 13768.004 Loc: 0.855\n",
      "Step 1960: Loss: 13768.004 Loc: 0.855\n",
      "Step 1961: Loss: 13768.004 Loc: 0.855\n",
      "Step 1962: Loss: 13768.004 Loc: 0.855\n",
      "Step 1963: Loss: 13768.004 Loc: 0.855\n",
      "Step 1964: Loss: 13768.004 Loc: 0.855\n",
      "Step 1965: Loss: 13768.004 Loc: 0.855\n",
      "Step 1966: Loss: 13768.004 Loc: 0.855\n",
      "Step 1967: Loss: 13768.004 Loc: 0.855\n",
      "Step 1968: Loss: 13768.004 Loc: 0.855\n",
      "Step 1969: Loss: 13768.004 Loc: 0.855\n",
      "Step 1970: Loss: 13768.004 Loc: 0.855\n",
      "Step 1971: Loss: 13768.004 Loc: 0.855\n",
      "Step 1972: Loss: 13768.004 Loc: 0.855\n",
      "Step 1973: Loss: 13768.004 Loc: 0.855\n",
      "Step 1974: Loss: 13768.004 Loc: 0.855\n",
      "Step 1975: Loss: 13768.004 Loc: 0.855\n",
      "Step 1976: Loss: 13768.004 Loc: 0.855\n",
      "Step 1977: Loss: 13768.004 Loc: 0.855\n",
      "Step 1978: Loss: 13768.004 Loc: 0.855\n",
      "Step 1979: Loss: 13768.004 Loc: 0.855\n",
      "Step 1980: Loss: 13768.004 Loc: 0.855\n",
      "Step 1981: Loss: 13768.004 Loc: 0.855\n",
      "Step 1982: Loss: 13768.004 Loc: 0.855\n",
      "Step 1983: Loss: 13768.004 Loc: 0.855\n",
      "Step 1984: Loss: 13768.004 Loc: 0.855\n",
      "Step 1985: Loss: 13768.004 Loc: 0.855\n",
      "Step 1986: Loss: 13768.004 Loc: 0.855\n",
      "Step 1987: Loss: 13768.004 Loc: 0.855\n",
      "Step 1988: Loss: 13768.004 Loc: 0.855\n",
      "Step 1989: Loss: 13768.004 Loc: 0.855\n",
      "Step 1990: Loss: 13768.004 Loc: 0.855\n",
      "Step 1991: Loss: 13768.004 Loc: 0.855\n",
      "Step 1992: Loss: 13768.004 Loc: 0.855\n",
      "Step 1993: Loss: 13768.004 Loc: 0.855\n",
      "Step 1994: Loss: 13768.004 Loc: 0.855\n",
      "Step 1995: Loss: 13768.004 Loc: 0.855\n",
      "Step 1996: Loss: 13768.004 Loc: 0.855\n",
      "Step 1997: Loss: 13768.004 Loc: 0.855\n",
      "Step 1998: Loss: 13768.004 Loc: 0.855\n",
      "Step 1999: Loss: 13768.004 Loc: 0.855\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000):\n",
    "    loss, grads = get_loss_and_grads(x_train)\n",
    "    optimizer.apply_gradients(zip(grads, normal.trainable_variables))\n",
    "    \n",
    "    loc_value = normal.loc.value()\n",
    "    print(\"Step {:03d}: Loss: {:.3f} Loc: {:.3f}\".format(i, loss, loc_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0be9bc25-7999-437c-b3e6-c58e2d11f3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Value: 0.8548658490180969\n",
      "Estimated Value: 0.8548658490180969\n"
     ]
    }
   ],
   "source": [
    "# Compare the true value and the estimated parameter\n",
    "\n",
    "print(f'True Value: {x_train.mean()}')\n",
    "print(f'Estimated Value: {normal.trainable_variables[0].numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ef4878-4a07-41f7-81d8-c1bbd5b8f1f0",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This article introduced the  Maximum Likelihood Estimation procedure both theoretically as well as in practice by computing an example in TensorFlow Probability. We started by stating the differences between the probability density function and the likelihood function. The first has the parameter 𝜃 fixed and the samples are the variables of interest. Conversely, in the case of the likelihood function,  it is the other way around, the data is fixed (i.e. observed) and the parameter 𝜃 is the variable that we want to learn. We then implemented a very simple example that allowed us to get a visual intuition of the shape of the likelihood function. Finally, we implemented a custom training procedure using TensorFlow Probability by defining a TensorFlow Variable, a negative log likelihood function and by applying the gradients.\n",
    "\n",
    "Next week, we will start building our first algorithms. See you then!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df4caf-5064-4106-a2a2-72118dcbfba4",
   "metadata": {},
   "source": [
    "# References and Materials\n",
    "\n",
    "[1] - [Coursera: TensorFlow 2 for Deep Learning Specialization](https://www.coursera.org/specializations/tensorflow2-deeplearning)\n",
    "\n",
    "[2] - [Coursera: Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)\n",
    "\n",
    "[3] - [TensorFlow Probability Guides and Tutorials](https://www.tensorflow.org/probability/overview)\n",
    "\n",
    "[4] - [TensorFlow Probability Posts in TensorFlow Blog](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
